Chapter 6: Image processing &
generating with LLM
The field of artificial intelligence (AI) and machine learning has witnessed significant
advancements in recent years, with the development of various tools and technolo-
gies that have transformed the way we approach complex tasks. There are a few
powerful AI models that has gained popularity among researchers and developers
due to its capabilities in computer vision and image processing. In this chapter, we
will provide a step-by-step guide on utilizing LLM for image visioning and image
processing tasks.
First of all, we will start from the Image visioning. Image visioning is a crucial aspect
of AI, with applications in various fields such as healthcare, finance, and education.
The ability to analyze and understand visual data has numerous benefits, including
improved decision-making, enhanced customer experience, and increased efficiency.
However, implementing image visioning tasks can be challenging, especially for
those without prior experience. This is where LLaVA-v1.6 comes in – a powerful
tool that simplifies the process of image visioning and provides accurate results.
Image visioning
LLaVA-v1.6 is a deep learning-based AI model that specializes in natural language
processing and computer vision. Developed by a team of researchers, LLaVA-v1.6 is
designed to analyze and understand visual data, including images and videos. The
model uses a combination of convolutional neural networks (CNNs) and recurrent
neural networks (RNNs) to extract features from images and generate text-based
descriptions.
LLaVA-v1.6 has a rich history, with its development dating back to 2019. Since
its inception, the model has undergone several updates, with the latest version
(v1.6) offering improved performance and accuracy. The key features of LLaVA-v1.6
include:
Chapter 6: Image processing & generating with LLM 212
Image analysis: LLaVA-v1.6 can analyze images and extract features such as objects,
scenes, and actions. Text generation: The model can generate text-based descrip-
tions of images, including captions and summaries. Object detection: LLaVA-v1.6
can detect objects within images, including people, animals, and vehicles.
Possibilities and Functionalities of LLaVA-v1.6
LLaVA-v1.6 has numerous possibilities and functionalities, making it a versatile tool
for various applications. Some of the key possibilities of LLaVA-v1.6 include:
Image classification: LLaVA-v1.6 can classify images into different categories, such
as objects, scenes, and actions. Object detection: The model can detect objects
within images, including people, animals, and vehicles. Image captioning: LLaVA-
v1.6 can generate text-based captions for images, including descriptions of objects,
scenes, and actions.
LLaVa architecture
In essence, LLaVa is like a smart assistant that can read and look at pictures at the
same time to understand what you need and give you helpful responses. From a
technical perspective, the LLaVa architecture is as shown below.
Figure 6.1
Chapter 6: Image processing & generating with LLM 213
The image was taken from the official documentation of LLaVA-v1.6.
The image describes the process of interacting with the LLaVa model where, user
input an image along with a text prompt that contains instructions. The text prompt
is processed directly by the language model’s tokenizer while the image is first
converted into tokens using a vision encoder. These tokens are then mapped to the
space of the language model.
Step-by-Step Example: Utilizing LLaVA-v1.6 for Image
Visioning
In this section, we will provide a step-by-step example of using LLaVa-v1.6 for image
vision tasks. We will cover a few straightforward examples, such as describing
images and extracting code or text from them.
Prerequisites
Software: You need to have Ollama installed and configured locally to use LLaVa-
v1.6. If you need instructions on how to install Ollama, please refer to the chapter
1.
Hardware: A computer with a dedicated graphics card is recommended for faster
processing. However, for educational purposes, an 8-core CPU will be sufficient.
Step 1. Installing and Setting Up LLaVA-v1.6
Run the following command in your terminal:
ollama run llava:34b
This command will download and install the LLava 1.6 LLM into your local Ollama
setup. Please note that the llava:34b model requires more than 20 GB of free memory
to run.
Step 2. Pre-processing and Preparing Image Data
Create a directory to store images by running:
Chapter 6: Image processing & generating with LLM 214
mkdir images
Then, copy a few images into the directory you just created. You can do this by using
the cp command, for example:
cp /path/to/your/images/* images/
Step 3. Implementing LLaVA-v1.6 for Image Visioning
Now, you can load the images into the LLaVa-v1.6 model and ask questions about
the images. Let’s start with a simple prompt what is in the image.
1. Describing images
Figure 6.2
Prompt: what is in the image /path_to_the_image
In the terminal where Ollama run, execute the following command:
Chapter 6: Image processing & generating with LLM 215
Figure 6.3
After a short wait, LLaVa returns the following answer:
The image shows a piece of furniture with a metal frame, which appears \
to be the footboard or headboard of a bed. The metal is dark in color a
nd there are visible bolts and other hardware components that suggest t
he bed frame is sturdy and securely assembled. There's no text present
in this image. If you have any questions about the type of bed frame or
need advice on furniture, feel free to ask!
Impressive result.
2. Understanding a digram or schema
Chapter 6: Image processing & generating with LLM 216
Figure 6.4
Prompt: describe the diagram /path_to_the_image
Result:
The diagram appears to be a flowchart or an organizational chart, possi\
bly representing a system or process with various components and data f
ields. It includes nodes connected by lines that suggest relationships
or workflow between the elements. Each node is labeled with a title suc
h as "Customer"
,
"Employee"
"Track"
,
, and others that seem to indicate
different categories or entities within the system being represented.
The diagram uses a dark background with white text and light-colored no\
des, which creates a contrast for readability. The lines connecting the
nodes are likely to represent the flow of information or the sequence
of steps in the process. Each node has fields on it that seem to corres
pond to specific data points, such as customer ID, name, email, and so
forth, suggesting that this diagram might be related to a software appl
Chapter 6: Image processing & generating with LLM 217
ication or database schema where different entities have their own attr
ibutes and relationships.
The image does not provide enough context to determine the exact nature\
of the system or process being represented in the flowchart, but it is
clear that it is designed to show the structure and interconnectivity
of various elements within that system.
Let’s try one more diagram.
Figure 6.5
Prompt: Describe the meaning of the diagram /path_to_the_image
Result:
The image you've shared appears to be a flowchart or diagram that repre\
sents an example of how data processing works. It seems to illustrate a
simple system where input data is processed and
results are generated based on predefined conditions.
Here's a breakdown of the elements in the diagram:
1. The input statement reads "How many employees live in Calgary?" This\
indicates that the system is designed to process information about emp
loyees who reside in Calgary.
2. There are two nodes labeled as 'Employee', which suggests that there\
might be multiple employee records or data points within the system.
3. Each 'Employee' node has fields such as 'ID'
'Name'
,
, and'Location'\
. The 'Location' field is presumably used to store where each employee
resides.
Chapter 6: Image processing & generating with LLM 218
4. The diagram includes an 'Employee' node with the 'Location' field se\
t to 'Calgary', which represents a specific data point within the syste
m.
5. There are two decision points in the flowchart: 'Model 1' and'Model\
2'
. These likely represent different algorithms or methods used for pr
ocessing the input data.
6. The output statement at the bottom of the diagram reads "There are 3\
employees living in Calgary"
. This suggests that after processing the
input data, the system has determined that
there are three employees who live in Calgary.
Overall, the meaning of this diagram is to illustrate a simple example \
of how an input question can be processed through a predefined model or
algorithm to generate an output answer based on the available data wit
hin a system. It's a basic representation of data processing and does n
ot include specific details about the inner workings of the models or a
lgorithms involved in the process.
The result is very impressive and accurate.
3. Extracting code from images
Chapter 6: Image processing & generating with LLM 219
Figure 6.6
Prompt: extract code from the image /path_to_the_image
Result:
python
while True:
byte_buffer.msg = stream_read_pending()
byte_buffer.stream.sleep(100)
if byte_buffer.msg == None:
continue
time.sleep(1 / 60)
for i in range(1,12):
byte_buffer.message = message + " " + str(i)
source_sequence.set("seq_" + byte_buffer.msg + "
_
" + str(len(by\
te_buffer.message)))
if source_sequence.length < 0:
print("Error: Source Sequence Length is less than zero")
It’s interesting that LLaVa extracted the pseudocode but returned it in Python
language.
Chapter 6: Image processing & generating with LLM 220
4. Reading text from the image
Figure 6.7
Prompt: extract text from the image /path_to_the_image
Result:
Chapter 6: Image processing & generating with LLM 221
Nine Presentation Lessons from Steve Jobs
1. Show Your Passion
2. One Sentence Summary of Product
3. The Third Rule
4. Use Simplicity to Win the Battle
5. Don't Hide Behind Bullet Points
6. Make Everything Personal and Relatable
7. Create a Hero for Customers
8 8. Tell Them Stories About How Your Product Changes Lives
9. Keep the Data Clean
1
1. Bring the Hero After a Short Break
2. Every Story Needs an Ending
3. Don't Read from Notes - Prepare, Practice, and Preach
4. Be Yourself and Help You Give Your Best Performance
5. Prepare to Give a Good Presentation
6. Use a Visual Aid for Better Understanding of the Product
7. Tell Them About How Your Product Changes Lives
8. Bring the Hero After a Short Break
9. Keep the Data Clean
10. Don't Read from Notes - Prepare, Practice, and Preach
11. Be Yourself and Help You Give Your Best Performance
12. Use a Visual Aid for Better Understanding of the Product
13. Create a Hero for Customers
14. Bring the Hero After a Short Break
15. Every Story Needs an Ending
16. Don't Hide Behind Bullet Points
17. Make Everything Personal and Relatable
18. Use Simplicity to Win the Story
19. Show Your Passion
20. One Sentence Summary of Product
21. The Third Rule
22. Bring the Hero After a Short Break
23. Don't Hide Behind Bullet Points
24. Make Everything Personal and Relatable
Chapter 6: Image processing & generating with LLM 222
The image is large, so LLaVa takes a few minutes to encode and decode it. However,
the result is spectacular.
Incorporating LLaVA into your application for image
processing
To utilize the power of the LLAVA model for image analysis, you can integrate
it into your projects in various ways. The simplest method is using the Ollama
Python API, but you can also use other frameworks such as LangChain. Below, we
provide a Python program that defines a function to process an image and generate a
description based on a given prompt. Here’s a step-by-step explanation of the code.
Before you start, make sure you have the necessary libraries installed and configured
in your development environment. As usual, we will use a Conda environment and
JupyterLab to develop the application.
Step 1. Install the required libraries
Please install the following Python libraries using the given commands. Ensure you
are in your Conda environment before running these commands.
!pip install --upgrade \
ollama \
pandas \
Pillow --quiet
Tip
Note that, if you are using Ollama on a separate dedicated server, you
need to export the following environment variable: export OLLAMA_-
HOST="YOUR_REMOTE_OLLAMA_IP_ADDRESS". Replace YOUR_REMOTE_OLLAMA_-
IP_ADDRESS with the actual IP of your Ollama server.
Step 2. Function Definition and Imports
Chapter 6: Image processing & generating with LLM 223
from PIL import Image
from io import BytesIO
import glob
import pandas as pd
import os
from io import BytesIO
# processing the images
def process_image(image_file_path, prompt):
print(f"\nProcessing {image_file_path} file \n")
The code imports the necessary modules: Image from the Python Imaging Library
(PIL) and BytesIO from the io module. It then defines a function called process_-
image that takes two arguments: image_file_path and prompt.
Step 3. Opening and Displaying the Image
image = Image.open(image_file_path)
display(image)
The function opens the image file specified by image_file_path and displays it.
Step 4. Converting the Image to Bytes
with image as img:
with BytesIO() as buffer:
img.save(buffer, format='PNG')
image_bytes = buffer.getvalue()
This section converts the image into a byte’s format. It uses a context manager to
open the image, saves the image into a buffer in PNG format, and retrieves the image
bytes from the buffer.
Step 5. Generating a Description of the Image
Chapter 6: Image processing & generating with LLM 224
# Generate a description of the given image
for response in generate(model='llava:34b'
,
prompt=prompt,
images=[image_bytes],
stream=True):
# Print the response to the console
print(response['response'], end='', flush=True)
This code generates a description of an image using the LLaVA model, streams the
response in real time, and prints it chunk by chunk to the console.
Step 6: Invoking the Function
file_name="./Text2SQL.png"
prompt="Describe the diagram"
# invoke process image function
process_image(file_name, prompt)
Finally, the code sets the file_name to the path of the image and the prompt to the
description prompt. It then calls the process_image function with these arguments.
Tip
The complete source code of the program is available in the GitHub
repository.
If you run the program in Jupyter, you should see a result similar to the one displayed
below.
Chapter 6: Image processing & generating with LLM 225
Figure 6.8
In summary, LLaVA-v1.6 is a powerful AI model that offers numerous possibilities
and functionalities for image visioning tasks. With its ability to analyze and
understand visual data, LLaVA-v1.6 has numerous applications in various fields,
including healthcare, finance, and education. While the model has some limitations,
it is a valuable tool for researchers and developers looking to explore the possibilities
of AI.
Image processing
Recent advancements in AI have made it possible to generate images from text
descriptions, with OpenJourney being one of the standout tools. This section will
guide you through the process of creating text-to-image content using OpenJourney
LLM, with examples to help explain the steps.
What is OpenJourney?
OpenJourney is an opensource MidJourney trained model which canbe used as a local
hosted LLM version. Under the hood, it is a stable Diffusion 1.5 model fine-tuned by
Chapter 6: Image processing & generating with LLM 226
company PromptHero on over 124,000 Midjourney v4 images. The aim of the model
is to produce high-quality artwork from text prompts. Today, OpenJourney is one of
the most popular text-to-image models, widely used by researchers, developers, and
artists.
OpenJourney has been widely adopted in various industries, including art, design,
advertising, and education. The model has been used to generate images for various
applications, including:
• Art and design: OpenJourney has been used to generate images for art and
design projects, including paintings, sculptures, and installations.
• Advertising: OpenJourney has been used to generate images for advertising
campaigns, including product images and promotional materials.
• Education: OpenJourney has been used to generate images for educational
materials, including textbooks and online courses.
How Text to Image Works
As I mentioned before Openjourney based on Stable Diffusion 1.5 which is a text-
to-image generation model that creates images based on text prompts. It works by
gradually refining a noisy image until it matches the given description. Here’s a brief
overview of how it works:
1. Text Encoding: The input text prompt is first converted into a numerical
representation using a pre-trained language model.
2. Diffusion Process: The model starts with a random noise image and iteratively
denoises it, guided by the encoded text. Each step of the diffusion process moves
the image closer to the desired outcome, gradually adding more details that
align with the text prompt.
3. Image Generation: The final stage involves post-processing, where the noise is
reduced over many steps, the image becomes clearer, eventually resulting in a
fully formed image that matches the input description.
The entire image generation process is illustrated below.:
Chapter 6: Image processing & generating with LLM 227
Figure 6.9
If you’re interested in learning more about the Stable Diffusion model in detail, please
refer to the reference section.
Model inputs and outputs
The OpenJourney model accepts various inputs, including a text prompt, an optional
starting image, image dimensions, and additional parameters to control the output.
It generates one or more images based on these inputs.
• Inputs:
– Prompt: The text prompt describing the desired image
– Image: An optional starting image from which to generate variations
– Width/Height: The desired dimensions of the output image
– Seed: A random seed to control the image generation
– Scheduler: The denoising scheduler to use
– Num Outputs: The number of images to generate
– Guidance Scale: The scale for classifier-free guidance
Chapter 6: Image processing & generating with LLM 228
– Negative Prompt: Text to avoid in the output image
– Prompt Strength: The strength of the prompt when using an init image
– Num Inference Steps: The number of denoising steps
• Outputs:
– Image(s): One or more generated images, returned as a array of images
Next, we will provide step-by-step example of how to generate an image using
openjourney-v4. We will use a text prompt and walk through the process of encoding
the text, generating the image, and post-processing the output.
Warning:
Image generation is a resource-intensive process. If you have CUDA-
compatible GPUs, we recommend installing the latest driver from the
Nvidia website to take full advantage of their performance. Additionally,
you can leverage Google Colab’s hardware acceleration capabilities by
utilizing its T4 GPU for image generation, which will help speed up the
process. For a complete example of how to generate images on Google
Colab using the T4 GPU, please refer to this link.
However, since not everyone has access to such hardware, so we will also provide
an example using the CPU. Fortunately, most of the code remains largely the same
for both CPU and GPU implementations, making it easier to adapt to your specific
setup.
Step 1. Installation of required libraries
To use OpenJourney, you’ll need Python and several libraries. First, ensure you have
Python installed, then install the necessary packages:
Chapter 6: Image processing & generating with LLM 229
! pip install --upgrade \\
diffusers \\
transformers \\
safetensors \\
sentencepiece \\
accelerate \\
bitsandbytes \\
torch \\
huggingface_hub --quiet
! pip install ipywidgets
The above pseudocode installs the required libraries for the program. The ! at the
beginning indicates that these commands are run in the shell, not in Python.
Note that the local installation process of OpenJourney can be complex due to
variations in operating systems and configuration. Therefore, we will use the
Hugging Face Hub API to access the OpenJourney model in the next steps.
Step 2. Login to Hugging Face Hub
Create a new Python script or notebook and import the required libraries:
import os
from huggingface_hub import login
login(token=os.environ.get("HF_TOKEN"))
Here,
• imports the necessary modules.
• Logs into the Hugging Face Hub using a token stored in an environment variable
HF_TOKEN.
Step 3. Import required modules and check CUDA availability
Chapter 6: Image processing & generating with LLM 230
from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler, A\
utoencoderKL
import torch
# Check the CUDA availability
torch.cuda.is_available()
Here,
• Imports the DiffusionPipeline, DPMSolverMultistepScheduler, and
AutoencoderKL from the diffusers library, as well as the torch library.
• Checks if CUDA (GPU support) is available.
Step 4. Set Up the Diffusion Pipeline
Initialize the pipeline for the OpenJourney model. Make sure you have access to the
model files or the capability to download them:
model_id = 'prompthero/openjourney-v4'
pipe = DiffusionPipeline.from_pretrained(
model_id,
torch_dtype=torch.float32
)
pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler\
.config)
pipe.vae = AutoencoderKL.from_pretrained("stabilityai/sd-vae-ft-mse", t\
orch_dtype=torch.float32)
Here,
• Loads the pre-trained OpenJourney model into the pipe variable.
• Configures the scheduler and VAE (Variational Autoencoder) for the pipeline.
Step 5. Image Generation Settings
Create a text prompt that describes the image you want to generate. For instance:
Chapter 6: Image processing & generating with LLM 231
# Image generation input params
import random
prompt = 'Istanbul street, bosforas, old city, summer, sunny, hyper-det\
ailed, comprehensive cinematic, Atmosphere, Masterpiece, hyperdetailed
8k '
negative_prompt = 'blur haze, pencils, pens, fingers'
num_steps = 20
# variation number
num_variations = 1
prompt_guidance = 9
dimensions = (400, 600)
random_seeds = [random.randint(0, 65000) for
_
in range(num_variations)]
• Imports the random module.
• Sets up the prompt, number of steps for image generation, number of variations,
guidance scale, dimensions, and random seeds for generating multiple images.
Change the num_variations param if you wish to generate more images.
Step 6. Generate Images
images = pipe(prompt= num_variations * [prompt],
num_inference_steps=num_steps,
guidance_scale=prompt_guidance,
height = dimensions[0],
width = dimensions[1],
Negative_Prompt = negative_prompt,
num_outputs = 1,
generator = [torch.Generator().manual_seed(i) for i in ra\
ndom_seeds]
).images
print ('Images: '
, len(images) )
• Generates images based on the provided prompt and settings.
Chapter 6: Image processing & generating with LLM • Uses the random seeds to ensure variability in the generated images.
232
Figure 6.10
Running this code will generate an image of Istanbul based on the provided prompt.
The image is essentially an artwork created by the LLM. Keep in mind that the final
result may vary depending on your computer hardware and configuration settings.
However, the output should be an image of an old Istanbul street on a sunny summer
day. The entire source code of the Jupyter notebook is available on GitHub. Note
that
Tips for Better Results
1. Be Descriptive: The more details you provide in your text prompt, the better
the model can understand and generate the desired image. For getting ideas for
your next prompt engineering session, you can visit the web site.
Chapter 6: Image processing & generating with LLM 233
2. Experiment: Try different prompts and variations to see how the model
responds.
3. Fine-Tuning: If you have specific requirements, consider fine-tuning the model
with custom datasets.
OpenJourney is a powerful tool with the potential to transform various industries
through its ability to generate high-quality images from text prompts. By following
the steps in this section, you can create your own images and explore the many
applications of this model.
References
1. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-
resolution image synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp.
10684–10695).
2. Zhang, L., & Agrawala, M. (2023). Adding conditional control to text-to-image
diffusion models. arXiv preprint arXiv:2302.05543.
Conclusion
This chapter demonstrated how to use the LLaVA model to answer image-related
questions, describe diagrams and schemas, and generate detailed descriptions of
complex systems. The code examples provided a hands-on guide for integrating the
LLaVA LLM in image-related tasks. By leveraging this model, developers can unlock
new possibilities in image processing and analysis.
We also explored OpenJourney’s capabilities in generating high-quality images from
text prompts, guiding you through the process of building applications using the
OpenJourney API to create visually striking images.