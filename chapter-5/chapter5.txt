Chapter 5: Fine-tuning LLMs
Fine-tuning a Large Language Model refers to the process of adapting a pre-trained
LLM, like Llama or Mistral, for a specific task or domain by modifying its weights and
architecture. This is done to make the model more effective at a particular problem,
such as text classification, sentiment analysis, or document summarization.
In the fine-tuning process, you start with a pre-trained model, like LLaMA 3, which
has been trained on a vast amount of general knowledge from various sources. You
then fine-tune this model for a specific task, such as code generation or converting
text into SQL code.
To put it in plain English, imagine you have a super smart friend who knows a lot
about many topics but hasn’t specifically studied your area of interest, like hiking
or diving. This friend represents a pre-trained LLM model. Now, you want to
teach your friend more about your specific area of interest. You provide him with
examples and explanations related to hiking, for instance. As he learns from these
new examples, his understanding becomes more specialized and accurate in the
context of hiking.
We can visualize the entire process as shown below:
Chapter 5: Fine-tuning LLMs 155
Figure 5.1
LLM fine-tuning offers several benefits:
1. Cost-effective: Fine-tuning is less computationally expensive and time-
consuming compared to training a model from scratch.
2. Domain expertise: By adapting the model for your specific domain, you can
leverage its language understanding capabilities in that context.
3. Flexibility: Fine-tuned models can be adapted for various tasks and domains,
making them versatile assets.
However, fine-tuning a LLM is a resource-intensive process that requires specific
knowledge of the overall tuning process. Before investing time and resources into
fine-tuning an LLM, it’s essential to ensure that you really need to do so. Consider
fine-tuning your model when,
• You have a small dataset: Fine-tuning is suitable when you don’t have a large
amount of labeled data available for training from scratch.
• Domain-specific knowledge is crucial: When the task or domain requires spe-
cialized knowledge, fine-tuning can help adapt the model to better understand
the context.
Chapter 5: Fine-tuning LLMs 156
• Resources are limited: In cases where resources (e.g., GPU, compute power)
are constrained, fine-tuning offers a more efficient alternative.
In this chapter, we’ll provide a thorough overview of the fine-tuning process for
LLMs, giving you a solid understanding of the theoretical foundations. Then, we’ll
guide you through practical steps to tune an open-source model like Phi-2 for a
specific task. Throughout this chapter, our main goal is to maintain a balance
between theoretical concepts and practical applications. To achieve this, we’ll first
dive into the underlying theories and then move on to concrete examples and hands-
on exercises.
Info
Note that, fine-tuning a Large Language Model is a complex topic that
requires a comprehensive understanding of various concepts and tech-
niques. One chapter or brief overview is not sufficient to cover all aspects
of this process. If you’re interested in learning more about fine-tuning an
LLM, I recommend exploring additional resources dedicated to this specific
process.
Steps for Fine-tuning a pre-trained model
The fine-tuning process of large language models is more complex than what’s
depicted in Figure 5.1. As we’ve previously mentioned, fine-tuning involves training
a pre-trained model on a specific dataset related to your task, business requirements,
or domain. The general steps for fine-tuning an LLM are illustrated in Figure 5.2.
Chapter 5: Fine-tuning LLMs 157
Figure 5.2
While the details may vary depending on the library or framework you are using,
here’s an outline of general steps to guide you through the process:
1. Analyze your busniess requirements: Before fine-tuning LLM, it’s crucial
to analyze your business requirements to ensure the fine-tuning effort aligns
with your goals, maximizes resources, and delivers value. Here’s a structured
approach to analyzing your business requirements before fine-tuning:
• Define the Business Objective. Clearly define why you want to fine-tune
an LLM. Are you aiming to automate customer service, improve content
generation, or streamline internal processes? Identify the key business
problem or opportunity the model will address.
• Identify the Target Audience. Determine whether the fine-tuned LLM will
be customer-facing (e.g., chatbots, content generation) or used internally
(e.g., document processing, knowledge retrieval).
• Evaluate Data Availability. Determine whether you have enough high-
quality data for fine-tuning. For example, if you’re building a customer
Chapter 5: Fine-tuning LLMs 158
service chatbot, do you have enough chat transcripts or support tickets to
train the model effectively?
• Consider Cost and Resources. Fine-tuning large models requires significant
computational resources (e.g., GPUs or TPUs). Assess the budget for cloud
services or hardware to handle the fine-tuning process. Ensure your team
has the skills in machine learning, data processing, and model evaluation.
2. Choose a Pre-trined Model: Start by selecting a pre-trained model that fits
your needs. Common models include GPT, LLaMA, Mistral and others. The
choice depends on the task (e.g., text generation, classification, or translation)
and the availability of models. Ensure that the pre-trained model has been
trained on a large corpus of general data, which provides a strong foundation
for fine-tuning. Moreover, to compare different pre-trained models, refer to
benchmarks (or LLM Leaderboard) such as:
• MMLU : Evaluates the performance of language understanding capabilities.
• HellaSwag: Assesses the ability to generate coherent text.
• DROP: Tests the model’s capacity for reasoning and common sense.
By considering these factors and comparing pre-trained models through
established benchmarks, you can make an informed decision and select
the ideal model for your specific needs.
3. Setup the environment:
• Set up the necessary software, frameworks, and tools for fine-tuning.
Common libraries include Hugging Face’s Transformers, PyTorch, MLX
and TensorFlow.
• Fine-tuning requires significant computational resources, so it’s important
to use GPUs or TPUs for faster training. Cloud platforms like Google
Colab, Kaggle or AWS can provide access to such hardware.
4. Prepare your dataset:
• Gather or create a synthetic dataset that is relevant to the task you want
to fine-tune the model for. This could include labeled examples (e.g., text
with categories for classification, dialogue for chatbots).
Chapter 5: Fine-tuning LLMs 159
• You’ll need to prepare your dataset by splitting it into two categories:
Training and Testing. Training set portion of the dataset will be used to
train the LLM. On the other hand, Testing set part of the dataset will be
used to evaluate the performance of the fine-tuned model. - Ensure the
data is clean and pre-processed. For text-based tasks, this might involve
removing irrelevant information, tokenizing the text, or converting data
into the format required by the model (e.g., converting text to tokens).
• Organize the dataset in a format the model can understand, such as
input-output pairs for supervised learning tasks (e.g., input: text, output:
summary).
5. Fine-tuning configuration: Define hyperparameters such as learning rate,
batch size, and the number of training epochs. These may vary based on your
specific task.
6. Train the model: Train the model on the task-specific dataset. During this
step, the model’s parameters are adjusted based on the new data to make
it more effective for the target task. Monitor performance metrics like loss,
accuracy, or other relevant evaluation metrics. This helps in tracking progress
and determining when to stop training to avoid overfitting. You can consider
W&B platform for monitoring metrics during fine-tuning process.
7. Evaluate and Refine: After fine-tuning, evaluate the model on a separate
validation or test dataset to check its performance on unseen data. Use task-
specific metrics such as accuracy, precision, ROUGE, F1 score, or BLEU score
to assess how well the fine-tuned model performs. Human evaluation is also
commonly used to gauge the model’s effectiveness.
8. Save & deploy the final model: Once fine-tuning is complete, save the
fine-tuned model, including all relevant weights and configurations. Deploy
the fine-tuned model into production or integrate it into your application,
such as a chatbot, recommendation system, or document processing pipeline.
After deployment, continue to monitor the model’s performance in real-world
scenarios and retrain or fine-tune further if needed.
If the initial fine-tuning process doesn’t deliver the desired results on production,
there are several steps you can take to refine your model:
• Augmenting the Dataset. Consider augmenting your dataset by:
Chapter 5: Fine-tuning LLMs 160
– Adding More Training Data: Supplement your existing data with addi-
tional examples related to your specific task or domain.
– Refining the Dataset: Review and refine the existing dataset for improved
quality.
• Adjusting Hyperparameters. adjust hyperparameters as needed:
After evaluating the model’s performance,
– Learning Rate: Adjust the learning rate to optimize the model’s conver-
gence speed.
– Number of Training Epochs: Increase or decrease the number of training
epochs to achieve better results.
• Fine-Tuning Again. Once you’ve made the necessary adjustments, fine-
tune the model again using the updated dataset and hyperparameters. This
process might require several iterations, but it’s a normal part of the fine-tuning
workflow.
Fine-tuning technics
So far, we have learned about the steps required for fine-tuning an LLM. However, a
lot of technical jargon is involved in the process of fine-tuning. During this process,
the model’s weights (refer to Chapter 2 for more details) are adjusted to better align
with the specific task or data. Fine-tuning techniques focus on finding the optimal
way to modify these weights so the model can perform well on a target task without
losing the general knowledge gained during pre-training.
There are various fine-tuning techniques that can be applied to LLMs, depending on
the task, available data, and computational resources. In this section, we will discuss
the most prominent fine-tuning methods used for LLMs:
Full Fine-Tuning
In this approach the entire model’s weights are updated based on a task-specific
dataset. The model learns to adapt its internal representations to the new task. It
Chapter 5: Fine-tuning LLMs 161
demands significant memory and computational resources, similar to pre-training,
to handle storage and processing during training.
The weights of the model are updated using an optimization algorithm, typically
Adam or Stochastic Gradient Descent (SGD), based on the computed gradients.
Moreover, all layers of the model are updated. This includes early layers that
capture general language features and deeper layers that focus on more abstract
representations. The magnitude of these updates depends on the learning rate, a
hyperparameter that controls how large the adjustments are. Lower learning rates
prevent drastic changes to the weights, ensuring that the model retains valuable pre-
trained knowledge while adjusting to the new task.
Figure 5.3
Key Characteristics of Full Fine-Tuning:
• All layers are updated: In full fine-tuning, every weight in the model is adjusted,
allowing the model to deeply specialize in the new task.
Chapter 5: Fine-tuning LLMs 162
• Computationally expensive: Since all weights are updated, full fine-tuning
requires significant computational resources, especially for large models.
• Highly effective for domain-specific tasks: Full fine-tuning allows the model to
adapt comprehensively to the nuances of the task-specific data, leading to high
performance on specialized tasks.
• Risk of overfitting: Because the model is fully updated, there is a risk of
overfitting to the fine-tuning dataset, especially if it is small.
However, it produces the best task-specific performance because all layers are
adjusted.
Parameter-Efficient Fine-Tuning (PEFT)
This is one of the most common methods of fine-tuning LLMs today. With PEFT,
only a subset of the model’s parameters is updated. Instead of modifying all the
weights (as in full fine-tuning), PEFT focuses on adjusting a smaller subset of
parameters, often by introducing additional components or adapting specific layers.
During the process, the rest of the model is typically frozen, meaning it remains
unchanged. This allows the model to specialize in a new task without retraining the
entire network.
Key Characteristics of PEFT:
• Efficiency in Memory and Computation: PEFT significantly reduces the num-
ber of trainable parameters, which means less memory and computational
power is required. This is especially important for very large models, which
can be resource-intensive to fine-tune fully.
• Preserves Pre-trained Knowledge: Since most of the model’s parameters remain
unchanged, PEFT retains much of the general knowledge acquired during the
pre-training phase. This minimizes the risk of catastrophic forgetting, where
the model loses its ability to perform well on tasks it was initially trained on.
• Adaptability to Multiple Tasks: With PEFT, it’s possible to fine-tune a model for
multiple tasks by introducing small, task-specific components (e.g., adapters)
without altering the core model. This enables multi-task learning or domain
adaptation without needing to retrain the entire model from scratch.
Chapter 5: Fine-tuning LLMs 163
• Scalability: PEFT methods allow for efficient scaling across different tasks
or domains. This is particularly useful for organizations that need to deploy
LLMs across various applications (e.g., customer support, content creation,
translation) while keeping costs low.
Figure 5.4
While PEFT is highly efficient, it may not always achieve the same level of
performance as full fine-tuning. Since only part of the model is being adapted, it
may not capture the full complexity of certain tasks.
There are a few ways of achieving Parameter efficient fine-tuning. Low-Rank
Adaptation LoRA & QLoRA are the most widely used and effective.
LoRA (Low-Rank Adaptation)
LoRA is a technique that introduces low-rank update matrices (matrix) into the
attention layers of the transformer model. Instead of updating all weights in the
Chapter 5: Fine-tuning LLMs 164
attention layers, LoRA applies low-rank transformations to capture task-specific
variations.
LoRA injects small, trainable matrices that modify only a small subset of the model’s
internal components. The model learns task-specific representations by adjusting
only these low-rank matrices, while the bulk of the model’s parameters remain
frozen. Such a way, it reduces the number of trainable parameters significantly,
making fine-tuning more efficient without sacrificing much in terms of performance.
After LoRA fine-tuning for a specific task or use case, the original LLM remains
unchanged, while a much smaller LoRA adapter is created—often only a single-digit
percentage of the original LLM’s size (measured in MBs rather than GBs).
Info
A LoRA adapter is a smaller, task-specific version of the original LLM that
is created after LoRA fine-tuning. You can think of the adapter as a plugin
for the pre-trained model, typically accounting for only a small fraction
of the original LLM’s size. Multiple adapters can be generated for different
purposes. During inference, the adapter is combined with the original LLM
to produce accurate results, offering flexibility to deploy the same LLM for
various tasks.
Quantized LoRA (QLoRA)
QLoRA is a variant of the Low-Rank Adaptation (LoRA) technique that incorporates
model quantization to make fine-tuning even more memory and computationally
efficient. QLoRA combines the principles of LoRA, which fine-tunes small low-rank
matrices while freezing the rest of the model, with quantization.
Info
Quantization reduces the precision of a model’s parameters, such as con-
verting from 16-bit floating point to 4-bit integers. This process allows the
same LLM to be more flexible and efficient for different tasks.
Chapter 5: Fine-tuning LLMs 165
By using 4-bit quantization, QLoRA reduces the memory footprint of large language
models (LLMs) by representing their weights with lower precision. This significantly
decreases memory requirements and accelerates both training and inference.
Like LoRA, it adds small, trainable low-rank matrices to the model. These matrices
are fine-tuned for the specific task while the rest of the quantized model remains
frozen. Despite quantization, QLoRA can achieve performance close to full-precision
models, balancing efficiency and task accuracy.
The 4-bit quantization significantly reduces memory usage, making it possible to
fine-tune very large models even on smaller GPUs, such as the Google Colab T4
with 16 GB of GPU RAM.
Knowledge Distillation (KD)
In these methods a smaller student model is trained to mimic the behavior of a
larger teacher model. The teacher is often a fine-tuned or pre-trained LLM, and
the student learns to replicate its behavior on a task while being smaller and more
efficient. It reduces the size of the model while preserving much of its performance.
Most commonly, these types of LLMs are used in resource-constrained environments,
such as mobile devices.
As shown in Figure 5.5 (original source Arxiv), in knowledge distillation, a small
student model learns to mimic a large teacher model and leverage the knowledge
of the teacher to obtain similar or higher accuracy.
Chapter 5: Fine-tuning LLMs 166
Figure 5.5
Key Characteristics of knowledge distillation:
• Teacher-Student Paradigm: The process involves two models: the teacher and
the student. The teacher is typically a large, well-trained model that has been
fine-tuned on a specific task or dataset.
• Knowledge Transfer: The goal of KD fine-tuning is to transfer knowledge from
the teacher to the student. This is done by training the student on the same data
as the teacher, but with an additional “temperature” term in the loss function
that encourages the student to mimic the output probabilities (or logits) of the
teacher.
• Soft Targets: The student learns to predict soft targets, which are probability
distributions over the output classes, rather than hard labels. This allows the
student to learn more general and abstract representations.
• Improved Transfer Learning: KD fine-tuning improves transfer learning by
allowing the student to inherit the knowledge and representations learned by
the teacher. This enables the student to learn more efficiently from a smaller
dataset or with fewer resources.
Several techniques can be used to implement knowledge distillation fine-tuning:
• Softmax Temperature Scaling: This involves scaling the output of the teacher
model using a temperature parameter to produce softer targets.
Chapter 5: Fine-tuning LLMs 167
• Output Regularization: Use output regularization techniques such as dropout
or weight decay to encourage the student model to learn diverse and robust
representations.
• Teacher-Student Weight Sharing: Allow certain weights in the pre-trained LLM
(the teacher) to be shared with the new task or dataset, effectively transferring
knowledge from the teacher to the student.
Note that, KD fine-tuning is not a replacement for traditional fine-tuning methods,
but rather an additional tool to improve the performance and efficiency of LLMs.
Popular frameworks used for fine-tuning
LLMs
Several frameworks are commonly used for fine-tuning LLMs. These frameworks
provide the tools, libraries, and infrastructure needed to adapt pre-trained models
to specific tasks. As this book is written, the field of AI is rapidly evolving, with
new frameworks and libraries emerging to address changing needs. Despite these
changes, here’s a list of popular frameworks used for fine-tuning LLMs:
1. Hugging Face Transformers.
Hugging Face is one of the most popular frameworks for fine-tuning LLMs.
It provides pre-trained models, tokenizers, and easy-to-use APIs for adapting
models to various tasks (e.g., text classification, summarization, question-
answering).
Key features:
• allows downloading/uploading pre-trained models and datasets.
• provides easy-to-use APIs for loading, fine-tuning, and deploying models.
• seamless integration with PyTorch and TensorFlow.
• Trainer class simplifies fine-tuning by handling training loops and evalu-
ation.
2. PyTorch.
Chapter 5: Fine-tuning LLMs 168
PyTorch is another popular deep learning framework that allows fine-tuning
of LLMs using its high-level API. It’s often used in conjunction with the
transformers library from Hugging Face. It allows for custom fine-tuning of
LLMs by providing low-level control over the training process.
Key features:
• provides dynamic computation graphs.
• integration with Hugging Face Transformers and other high-level libraries.
• very strong community support with many tutorials and libraries.
3. TensorFlow.
TensorFlow is a widely used open-source machine learning framework that
supports fine-tuning LLMs. TensorFlow, with its high-level API Keras offers
scalability for large models and allows for distributed training.
Key features:
• supports for distributed training across GPUs.
• integration with TensorFlow Hub, which provides pre-trained models for
easy fine-tuning.
• extensive ecosystem for machine learning and deep learning.
4. Unsloth.
Unsloth is a Python library designed to make fine-tuning pre-trained language
models, particularly those from the Hugging Face Transformers library. It
provides significant improvements in memory efficiency and speed, making
it possible to fine-tune large models on smaller hardware. Unsloth leverages
techniques like 4-bit quantization, LoRA (Low-Rank Adaptation), and gradient
checkpointing to reduce memory usage while preserving performance. This
allows models to handle longer context lengths (>2048), which is particularly
useful for tasks requiring extended sequences.
Key features:
• provides an interface for efficiently fine-tuning pre-trained language mod-
els, using techniques such as knowledge distillation and model pruning.
• helps reduce the memory requirements of large models during training,
making it possible to train them on smaller machines or with less RAM.
Chapter 5: Fine-tuning LLMs 169
• can compress large models into more compact forms, making them easier
to store and transfer.
• allows users to combine multiple pre-trained models into a single ensemble
model, which can improve performance on certain tasks.
5. MLX framework.
MLX is a new player in the LLM community, offering an innovative array
framework for machine learning research on Apple silicon. Developed by
Apple’s machine learning research team, MLX is primarily designed to leverage
Apple silicon processors and GPUs for local training of LLMs.
Key features:
• supports composable function transformations for automatic differentia-
tion, automatic vectorization, and computation graph optimization.
• operations can run on any of the supported devices (currently the CPU and
the GPU).
• provides a Python API that closely resembles NumPy, allowing for seam-
less integration and compatibility.
If you’re an owner of a Mac device featuring Apple silicon processors (such as M1,
M2, or M3) with sufficient memory, MLX is an excellent choice for fine-tuning
LLMs.
Keep in mind that each framework described above has its own strengths and
advantages, which depend on factors such as model scale, resource availability, and
ease of use. Additionally, there are other frameworks like JAX, Flax, and Colossal-AI
that cater to cutting-edge research and large-scale fine-tuning needs.
Step-by-step example of fine-tuning an LLM
After completing the theoretical foundation of fine-tuning, we’re now ready to dive
into a practical example. The purpose of this example is to clearly illustrate each
Chapter 5: Fine-tuning LLMs 170
step of the fine-tuning process that we’ve learned so far. After discussing our options
among the authors, we decided to use the Hugging Face framework for this example
because it’s easy to learn and has excellent community support with comprehensive
documentation and tutorials.
However, if you already have a strong foundation in Python programming or own an
Apple device with an M-series processor, you may want to consider using the Unsloth
or MLX frameworks instead. These alternatives offer more advanced capabilities and
are well-suited for developing a fine-tuning environment from scratch.
Prerequisites
Before we begin, let’s also review the prerequisites required to complete this example.
Environment Description
Cloud: Google Colab 16 Gb GPU (Т4) RAM. Pro Subscription not
needed.
Cloud: Kaggle 16 Gb GPU (Т4) RAM
Hugging Face Access Tokens
Network Connected to internet
Info
If your workstation/notebook has a graphics processing unit (GPU) with
at least 16 GB of RAM, you can attempt to run the notebooks locally. The
Python notebooks are available on our GitHub repository in Chapter 5.
For this example, we’re utilizing the Google Colab cloud environment with a free
subscription to run the notebooks. To provide a comfortable starting point, we’ve
taken an existing fine-tuning LLM example from Kaggle, adapted it for our dataset,
and modified it to run on Google Colab. Additionally, we’ve divided the entire source
code into two separate Python notebooks, which should help prevent out-of-memory
errors. The majority of the code remains unchanged and is available at this link.
Chapter 5: Fine-tuning LLMs 171
For a clear, step-by-step explanation of the entire process, we have divided it into
manageable parts. Although the sequence of steps remains the same.
Part 1. Analyze business requirements, choosing a base
model and environment setup
Let’s assume we’ve received a business proposal from a small private company
operating in market researching, which is interested in leveraging AI to enhance
their daily operations.
Step 1: Analyze your business requirements
The proposal outlines the development of a complex system capable of summarizing
technical articles while preserving their original meaning and readability. The
system will be deployed on a private cloud infrastructure and built using open-
source technologies. The generated summaries will be used in weekly newsletters.
Given that only 3 people will be using the system, we’re looking to create a highly
maintainable and cost-effective solution.
After conducting brief research, we arrived at the following conclusions:
1. The core of the system will be a local Language Model Inference engine.
2. We propose using an open-source, general-purpose LLM.
3. To achieve our goals, the LLM will need to be fine-tuned for understanding
technical articles and reports.
Step 2: Choose a Pre-trained model
As we can leverage open-source pre-trained models, we utilized the Hugging Face
Open LLM Leaderboard to compare the functionalities of various LLMs. After
reviewing the options, we decided to use the Microsoft Phi-2 as our base model. This
choice was driven by its impressive training dataset size (2.7 billion parameters) and
widespread adoption in Question Answering applications, despite its compact size of
only 5 GB. However, as the model excels at Question Answering but struggles with
text generation, we will need to fine-tune it to suit our specific requirements.
Chapter 5: Fine-tuning LLMs 172
Warning
Please note that, in practice, these steps would typically involve breaking
down into several sub-steps, including thorough research and comparison
of results against other LLMs. Additionally, to make a final decision, it
would be beneficial to test the LLMs on a test dataset using local or cloud-
based inference.
Step 3: Setup the environment
Since we don’t have access to sophisticated hardware equipped with GPUs, we will
utilize Google Colab as an environment for training our LLM. Google Colab provides
a generous allocation of 16 GB of GPU RAM for a few hours each day, which should
be sufficient for our needs.
Info
Note that multiple Google accounts can be used with Google Colab,
allowing you to access sufficient time slots on a weekly basis to complete
your routine tasks.
To get started, simply log in to Colab using any Google account. Next, create a new
notebook and navigate to the “Change runtime” section, where you can select the T4
runtime type.
Add the following code snippet to the first cell of your notebook:
!pip install -U bitsandbytes transformers peft accelerate datasets scip\
y einops evaluate trl rouge_score
The above command installs a set of Python libraries for natural language processing.
Here’s a breakdown of each library:
Chapter 5: Fine-tuning LLMs 173
• bitsandbytes: A library that helps optimize memory usage for large models by
using 8-bit precision during training, which reduces the computational load
and memory consumption without a significant loss in model accuracy.
• transformers: Developed by Hugging Face, this is a powerful library that
provides pre-trained models and tools for NLP tasks such as text classification,
translation, and question answering. It includes models like BERT, T5, etc.
• peft (Parameter-Efficient Fine-Tuning): A library by Hugging Face for
fine-tuning large models efficiently by freezing most of the parameters and
training only a small subset of them.
• accelerate: A library that simplifies distributed training and mixed-precision
training across multiple devices (GPUs/CPUs) and scales the training of large
models seamlessly.
• datasets: Also developed by Hugging Face, this library provides an easy way to
access, manipulate, and process large datasets. It includes thousands of datasets
commonly used in machine learning and NLP.
• scipy: A library for scientific computing in Python that provides functions
for optimization, integration, interpolation, eigenvalue problems, and other
advanced mathematical operations.
• einops: A tool for tensor manipulation and transformation in deep learning.
It allows you to perform complex reshaping of multi-dimensional arrays with
simple syntax, which is useful for working with image and text data.
• evaluate: A Hugging Face library designed to simplify the evaluation of
machine learning models. It provides metrics and tools for evaluating models’
performance on various tasks.
• trl (Transformers Reinforcement Learning): A library that provides tools
and models for combining reinforcement learning with transformers, helping
to optimize language models through reinforcement learning techniques.
• rouge_score: A library for computing ROUGE scores, which are commonly
used to evaluate the quality of text generation tasks like summarization.
ROUGE compares the overlap of n-grams between the generated text and
reference text.
Chapter 5: Fine-tuning LLMs 174
Info
Please ignore the warning if you encounter an error message indicating that
pip’s dependency resolver is unable to account for all installed packages,
leading to a dependency conflict.
Now, let’s load the necessary libraries to get started.
from datasets import load_dataset
from transformers import (
AutoModelForCausalLM,
AutoTokenizer,
BitsAndBytesConfig,
HfArgumentParser,
AutoTokenizer,
TrainingArguments,
Trainer,
GenerationConfig
)
from tqdm import tqdm
from trl import SFTTrainer
import torch
import time
import pandas as pd
import numpy as np
from huggingface_hub import interpreter_login
interpreter_login()
It will import several necessary components and libraries from Hugging Face, which
are required for further operations. Note that, we also import torch which is the core
of PyTorch, a deep learning framework. It provides functionality for creating and
manipulating tensors (multi-dimensional arrays) and is used extensively for building
and training neural networks.
Chapter 5: Fine-tuning LLMs 175
The last line of the code calls the interpreter_login() function, which prompts the
user to log in to Hugging Face by providing an API key. After logging in, the user
can access resources on the Hugging Face Hub.
Figure 5.6
To proceed, please enter your Hugging Face API key into the input field (as shown
in Figure 5.6) and then press Enter.
Part 2. Exploring the training dataset
At this point, your environment is set up and ready to use. You can now start working
with your dataset. After conducting a brief search on the Hugging Face dataset
repository, we found a suitable dataset for our needs: yasminesarraj/texts_summary.
This dataset contains technology articles along with their summaries, which aligns
perfectly with what we need.
Chapter 5: Fine-tuning LLMs 176
Figure 5.7
The dataset is divided into two parts, or splits: train and test. Each split contains
three columns (Unnamed:0, text, summary) and 47 rows of data. The total size of the
dataset is approximately 2.48 MB, which is a good fit for our use case.
Step 1: Load and explore the dataset
Load the dataset with the following code:
huggingface_dataset_name = "yasminesarraj/texts_summary"
dataset = load_dataset(huggingface_dataset_name)
print (dataset)
After executing this notebook cell, you should see an output that looks like the one
shown below:
Chapter 5: Fine-tuning LLMs 177
DatasetDict({
train: Dataset({
features: ['Unnamed: 0', 'text', 'summary'],
num_rows: 47
})
test: Dataset({
features: ['Unnamed: 0', 'text', 'summary'],
num_rows: 47
})
})
Based on the dataset, we’ll reserve the train split for model training and use the test
split for evaluation.
To explore the dataset further, you might consider using other dataset functions, such
as:
Function Description
dataset.features will return data types of the columns
{'Unnamed: 0': Value(dtype='int64',
id=None),<br> 'text':
Value(dtype='string', id=None),<br>
'summary': Value(dtype='string',
id=None)}
dataset‘summary’21] will return the summary column of the row 21
Step 2: Model Fine-tuning configuration
First, we’ll create a BitsAndBytes configuration that loads our model in 4-bit format.
This quantization technique will significantly reduce memory consumption at the
cost of some accuracy. To implement this, add the following code and execute the
cell.
Chapter 5: Fine-tuning LLMs 178
compute_dtype = getattr(torch, "float16")
bnb_config = BitsAndBytesConfig(
load_in_4bit=True,
bnb_4bit_quant_type='nf4'
,
bnb_4bit_compute_dtype=compute_dtype,
bnb_4bit_use_double_quant=False,
)
Here’s a breakdown of the key points in the code above:
• compute_dtype = getattr(torch, "float16") uses the Python function getattr
to retrieve the data type float16 from the torch library (PyTorch). float16
refers to half-precision floating-point numbers, which take up less memory
and are faster to compute compared to the more commonly used 32-bit (single
precision) floats.
• parameter load_in_4bit=True in BitsAndBytesConfig enables 4-bit quantiza-
tion, a method of representing the model’s weights and activations using only 4
bits per value, significantly reducing memory usage compared to the standard
16-bit or 32-bit representation.
• bnb_4bit_quant_type='nf4' sets the quantization type to nf4 (Normalized
Float 4). nf4 is a specific quantization format designed for better accuracy in
4-bit quantized models compared to standard 4-bit quantization.
• bnb_4bit_use_double_quant=False disables double quantization, a technique
where two levels of quantization are applied (usually during training and
inference). Double quantization can provide additional memory savings, but
disabling it may preserve model accuracy.
Info
If you’re planning to train a large model with a massive dataset, consider
experimenting with the bnb_4bit_use_double_quant parameter set to True
to conserve memory.
Step 3: Loading the pre-trained model
Chapter 5: Fine-tuning LLMs 179
As previously selected, we’ll use Phi-2 from Microsoft as our base pre-trained model
for training. Now, let’s load this model using a BitsAndBytes configuration.
model_name='microsoft/phi-2'
device_map = {"": 0}
original_model = AutoModelForCausalLM.from_pretrained(model_name,
device_map=device\
_map,
quantization_conf\
ig=bnb_config,
trust_remote_code\
=True,
use_auth_token=Tr\
ue)
Here,
• microsoft/phi-2 is a specific model available on Hugging Face’s Model Hub
under the Microsoft namespace.
• device_map = {"": 0} specifies which device (CPU or GPU) the model should
be loaded onto. In our case, we will try to use the both device: CPU and GPU.
Step 4: Dataset preparation and tokenization
A single row of the dataset is represented by a dictionary containing three key-value
pairs: Unnamed: 0, text, and summary. Let’s examine an individual row of data using
the following code:
row_21 = dataset[21]
print (row_21)
The result should be:
Chapter 5: Fine-tuning LLMs 180
{'Unnamed: 0': 21,
'text': ' Notice\nThis work has been submitted to the IEEE for possibl\
e publication.\nCopyright may be transferred without notice, after whic
h this version (...TRUNCATED) '
,
'summary': ' This paper proposes Swim, a new activation function for r\
einforcement learning and robotics tasks. It is tested on MuJoCo's loco
motion continuous control tasks (...TRUNCATED)'
}
We need to convert the dataset into tokens. For this, you can use the Hugging Face
Transformers library, which simplifies the process. The function performs several
tasks beyond tokenizing the text:
• Tokenizes the input text;
• Converts the output to PyTorch tensors;
• Pads the inputs to ensure uniform length;
Add the following code snippet to a new cell:
tokenizer = AutoTokenizer.from_pretrained(
model_name,
trust_remote_code=True,
padding_side="left"
,
add_eos_token=True,
add_bos_token=True,
use_fast=False)
tokenizer.pad_token = tokenizer.eos_token
This code sets up a tokenizer using the Hugging Face AutoTokenizer class, which
automatically selects and loads a tokenizer corresponding to the specified pre-trained
model. In a nutshell, the tokenizer processes text input by converting it into tokens,
which are then fed into the model for tasks like text generation or classification. The
key parameters are:
Chapter 5: Fine-tuning LLMs 181
• padding_side="left": This specifies that padding (extra tokens added to make
the sequence the same length as other sequences in a batch) will be added to
the left side of the input. Left-padding is commonly used for models like GPT,
Llama and other causal language models, where the attention is focused on the
right side (recent tokens) of the sequence. It also optimize the memory usage
during training.
• add_eos_token=True: EOS stands for end of sequence. This parameter tells the
tokenizer to automatically append an EOS token (usually indicating the end of
the sentence or sequence) to each input sequence. This helps the model know
when the input ends.
As an experiment to see what the tokenized text looks like, you can execute the
following pseudocode to print out the tokens.
tokenizer.encode("Getting started with Generative AI!")
This should produce an output similar to the following:
[50256, 20570, 2067, 351, 2980, 876, 9552, 0]
We can also set the maximum length of the tokens:
tokenizer.encode("Getting started with Generative AI!", padding='max_le\
ngth', max_length=10)
Result:
[50256, 20570, 2067, 351, 2980, 876, 9552, 0, 50256, 50256]
We can also get PyTorch tensors directly from the tokenizer:
Chapter 5: Fine-tuning LLMs 182
tokenizer.encode("Getting started with Generative AI!"
padding='max_length'
,
max_length=10,
return_tensors="pt")
,
result
tensor([[50256, 20570, 2067, 351, 2980, 876, 9552, 0, 50256,\
50256]])
The above code tokenizes the string Getting started with Generative AI! by
converting it into a PyTorch tensor of token IDs with a fixed length of 10 tokens.
If the tokenized sequence has fewer than 10 tokens, padding is added to reach the
length; if it’s longer, it gets truncated. The final output is a tensor ready to be fed
into a PyTorch model for tasks like text classification or generation.
Add one more Tokenizer for evaluating dataset to a new cell.
eval_tokenizer = AutoTokenizer.from_pretrained(
model_name,
add_bos_token=True,
trust_remote_code=True,
use_fast=False
)
eval_tokenizer.pad_token = eval_tokenizer.eos_token
We also need a function for generating text from an input prompt using a pre-trained
language model. The function should tokenize the input prompt, generates new
tokens from the model, and decodes the result into readable text. Add the following
function to a new cell.
Chapter 5: Fine-tuning LLMs 183
def gen(model,p, maxlen=100, sample=True):
toks = eval_tokenizer(p,
return_tensors="pt"#,
#truncation=True,
#max_length=512
)
res = model.generate(**toks.to("cuda"),
max_new_tokens=maxlen,
pad_token_id=tokenizer.eos_token_id,
do_sample=sample,
num_return_sequences=1,
temperature=0.1,
num_beams=1,
top_p=0.95,
).to('cpu')
return eval_tokenizer.batch_decode(res, skip_special_tokens=True)
This Python function gen takes a prompt and generates a sequence of text based on
it using a pre-trained language model (in our case, it’s Phi-2). It does the following:
• Tokenizes the input prompt into tokens.
• Generates a sequence of up to maxlen new tokens, with optional sampling to
add randomness.
• Decodes the generated tokens back into human-readable text.
The function allows you to control how deterministic or creative the output is (via
parameters like temperature and sample), and uses efficient hardware (GPU) to speed
up the generation process.
Step 5: Test the base model text generation, a simple smoke test
Add the following code and run the cell.
Chapter 5: Fine-tuning LLMs 184
%%time
import textwrap
from transformers import set_seed
seed = 42
set_seed(seed)
index = 21
prompt = dataset['test'][index]['text']
summary = dataset['test'][index]['summary']
formatted_prompt = f"Instruct: Summarize the following scientific repor\
ts .\n{prompt}\nOutput:\n"
res = gen(original_model,formatted_prompt,100,)
output = res[0].split('Output:\n')[1]
prompt_to_print = textwrap.shorten(formatted_prompt, width=100, placeho\
lder="
...
")
dash_line = '
'
-
.join('' for x in range(100))
print(dash_line)
print(f'INPUT PROMPT:\n{prompt_to_print}')
print(dash_line)
print(f'SUMMARY FROM FILE:\n{summary}\n')
print(dash_line)
print(f'MODEL GENERATION SUMMARY:\n{output}')
print(dash_line)
The code retrieves the test split from the dataset and formats a prompt instructing
the model to summarize the text. The pre-trained model (Phi-2) generates a
summary based on the prompt, and this summary is compared with the reference
summary from the file. The input prompt, SUMMARY FROM FILE, and MODEL
GENERATION SUMMARY are printed side by side, allowing for a direct human
Chapter 5: Fine-tuning LLMs 185
comparison.
The code retrieves the test split from the dataset and generates a prompt that
instructs the model to summarize the text. The pre-trained Phi-2 model then uses
this prompt to generate a summary, which is compared to the reference summary
stored in the file. To facilitate direct comparison, the input prompt, the reference
summary (SUMMARY FROM FILE), and the model-generated summary are printed side
by side.
-----------------------------------------------------------------------\
----------------------------
INPUT PROMPT:
Instruct: Summarize the following scientific reports . Notice This work\
has been submitted to the...
-----------------------------------------------------------------------\
----------------------------
SUMMARY FROM FILE:
This paper proposes Swim, a new activation function for reinforcement \
learning and robotics tasks.
It is tested on MuJoCo's locomotion continuous control tasks and is us\
ed in conjunction with the
TD3 algorithm ...
-----------------------------------------------------------------------\
----------------------------
MODEL GENERATION SUMMARY:
f(x) =x
2(kxp
1+k2x2+1)
;
f0(x) =1
2(kxp
1+k2x2)
(p
1+k2x2)3+1)
(1)
where kis a constant that can be tuned or defined before
training. We pick a ksuch that it can support our analysis that
Chapter 5: Fine-tuning LLMs 186
Swim outperforms
-----------------------------------------------------------------------\
----------------------------
CPU times: user 9.67 s, sys: 417 ms, total: 10.1 s
Wall time: 19.9 s
If you examine the output closely, you’ll notice that there is a clear distinction
between the summary provided by the reference file and the one generated by the
model. This difference highlights the potential for fine-tuning the model to better
suit the specific task at hand.
Part 3. Dataset pre-processing and adapter
configuration
So far, we have loaded the dataset, tokenized the data, and conducted some initial
testing. However, the raw dataset cannot be directly used for model training or fine-
tuning without additional preparation. It is crucial to format the prompt in a way
that the model can understand.
Step 1: Pre-processing the dataset
To prepare the dataset for fine-tuning, there are two primary options: the Combined
Format and the Separated Format.
1. Combined Format: In the Combined Format, instruction, the input and output
sequences are combined into a single text field or string. This is often done by
appending the target (output) to the input (source) with specific delimiters or
instructions to differentiate between them.
{Instruction: "Summarize the following text.", Input: "The full text do\
cument goes here.", Output: "The summary goes here."}
In this format, the model is trained to generate the output (summary, in this case)
by predicting the next sequence of tokens after the Output: instruction. The entire
prompt is treated as a single entity by the model. This format has several advantages:
Chapter 5: Fine-tuning LLMs 187
• Easy to train for text generation models like Llama, Mistral and other general
purpose language models where everything is treated as a single string for
generation.
• Useful when you’re using pre-trained language models for text generation tasks
in a zero-shot or few-shot setting.
2. Separated Format: In the Separated Format, the input (source) and output
(target) sequences are kept in separate fields or columns. This format is typical
for tasks where a clear distinction between input and output is important, such
as in translation models or encoder-decoder architectures.
Instruction: Summarize the following text.
Input: [The full text document goes here.]
Output: [The summary goes here.]
In this format, the model explicitly knows which part of the data is the input and
which part is the target output. For models like transformers with separate encoder
and decoder, the input is processed through the encoder and the output is generated
from the decoder. Advantages of this format is as follows:
• Provides a clear distinction between the input and output, which is important
for some model architectures that require explicit separation.
• Allows more control over the pre-processing and tokenization of input and
output sequences independently.
In this example, we’ll use the Combined Format to prepare the dataset. We will also
leverage a few helper functions to facilitate the data preparation phase. Add the
following three functions to separate cells:
Chapter 5: Fine-tuning LLMs 188
def create_prompt_formats(sample):
"""
Format various fields of the sample ('instruction','output')
Then concatenate them using two newline characters
:param sample: Sample dictionnary
"""
INTRO_BLURB = "Below is an instruction that describes a task. Write\
a response that appropriately completes the request."
INSTRUCTION_KEY = "### Instruct: Summarize the following scientific\
reports."
RESPONSE_KEY = "### Output:"
END_KEY = "### End"
blurb = f"\n{INTRO_BLURB}"
instruction = f"{INSTRUCTION_KEY}"
input_context = f"{sample['text']}" if sample["text"] else None
response = f"{RESPONSE_KEY}\n{sample['summary']}"
end = f"{END_KEY}"
parts = [part for part in [blurb, instruction, input_context, respo\
nse, end] if part]
formatted_prompt = "\n\n"
.join(parts)
sample["text"] = formatted_prompt
return sample
This code defines the function create_prompt_formats, that formats various fields
from a sample dictionary to create a structured prompt for training a text generation
model. The function takes a dictionary (sample) with fields like text (input)
and summary (output) and formats them into a structured prompt. It also adds
instructions, labels the output, and provides clear boundaries for the model to
understand the task.
Chapter 5: Fine-tuning LLMs 189
def get_max_length(model):
conf = model.config
max_length = None
for length_setting in ["n_positions"
,
"max_position_embeddings"
eq_length"]:
max_length = getattr(model.config, length_setting, None)
if max_length:
print(f"Found max lenth: {max_length}")
break
,
"s\
if not max_length:
max_length = 1024
print(f"Using default max length: {max_length}")
return max_length
def preprocess_batch(batch, tokenizer, max_length):
"""
Tokenizing a batch
"""
return tokenizer(
batch["text"],
max_length=max_length,
truncation=True,
)
This code contains two functions: get_max_length and preprocess_batch. These
functions handle the task of determining the maximum sequence length a model can
handle and then tokenizing a batch of text data accordingly.
The key point of the preprocess_batch function is that it ensures tokenized se-
quences conform to the model’s maximum length, truncating longer sequences as
needed. This prepares the text for input to the model.
Chapter 5: Fine-tuning LLMs 190
from functools import partial
# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/t\
rainer.py
dataset):
def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, \
"""Format & tokenize it so it is ready for training
:param tokenizer (AutoTokenizer): Model Tokenizer
:param max_length (int): Maximum number of tokens to emit from toke\
nizer
"""
# Add prompt to each sample
print("Preprocessing dataset...")
dataset = dataset.map(create_prompt_formats)#, batched=True)
_preprocessing_function = partial(preprocess_batch, max_length=max_\
length, tokenizer=tokenizer)
dataset = dataset.map(
_preprocessing_function,
batched=True,
remove_columns=['Unnamed: 0'
'text'
,
,
'summary'],
)
# Filter out samples that have input_ids exceeding max_length
dataset = dataset.filter(lambda sample: len(sample["input_ids"]) < \
max_length)
# Shuffle dataset
dataset = dataset.shuffle(seed=seed)
return dataset
The preprocess_dataset function defines the final step in preparing the dataset for
training a LLM for summarization tasks. It tokenizes and formats the data, handles
filtering as needed, and shuffles the data to ensure it is ready for model training.
Chapter 5: Fine-tuning LLMs 191
The function also randomly shuffles the dataset to ensure that the order of samples
is not biased during training. Using a seed ensures reproducibility—each time the
function runs with the same seed, it produces the same shuffled order.
Info
You don’t need to create a new Colab notebook from scratch. Instead, you
can upload the existing notebook LLM_tuning_for_text_summary.ipynb
from the chapter-5 folder of the GitHub repository directly into Colab by
navigating to File > Upload Notebook. This will allow you to run the code
cell-by-cell without having to copy it into a new notebook.
At this stage, the dataset (train, test) is prepared for pre-processing. To proceed, call
the required functions using the following code:
max_length = get_max_length(original_model)
print(max_length)
train_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset[\
'train'])
eval_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['\
test'])
For your reference, you can use the following code to print the shape of the dataset:
print(f"Shapes of the datasets:")
print(f"Training: {train_dataset.shape}")
print(f"Validation: {eval_dataset.shape}")
print(train_dataset)
Result:
Chapter 5: Fine-tuning LLMs 192
Shapes of the datasets:
Training: (9, 2)
Validation: (9, 2)
Dataset({
features: ['input_ids'
num_rows: 9
,
'attention_mask'],
})
There is one final important function to mention, which prints the number of
trainable model parameters.
def print_number_of_trainable_model_parameters(model):
trainable_model_params = 0
all_model_params = 0
for _, param in model.named_parameters():
all_model_params += param.numel()
if param.requires_grad:
trainable_model_params += param.numel()
return f"trainable model parameters: {trainable_model_params}\nall \
model parameters: {all_model_params}\npercentage of trainable model par
ameters: {100 * trainable_model_params / all_model_params:.2f}%"
print(print_number_of_trainable_model_parameters(original_model))
The function print_number_of_trainable_model_parameters calculate and prints
the number of trainable parameters in a LLM, compares them to the total number
of parameters in the model, and computes the percentage of trainable parameters.
This is useful when working with large models that may have a mix of frozen (non-
trainable) and trainable parameters, such as PEFT method.
If you execute this cell, you can expect results similar to those shown below:
trainable model parameters: 262364160
all model parameters: 1521392640
percentage of trainable model parameters: 17.24%
Chapter 5: Fine-tuning LLMs 193
The result percentage of trainable model parameters: 17.24% indicates
that, approximately 17.24% of the pre-trained model’s parameters might need to be
retrained.
Step 2: Setup PEFT for fine-tuning
Now that the dataset preparation phase is complete, we can proceed to prepare
the model for QLoRA. The next portion of the code demonstrates how to use
Parameter-Efficient Fine-Tuning (PEFT) with QLoRA to fine-tune a pre-trained
language model in a memory-efficient manner. The process involves creating a
QLoRA configuration, preparing the model for low-bit training, and then applying
the QLoRA modifications to the model.
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_tra\
ining
config = LoraConfig(
r=32, #Rank
lora_alpha=32,
target_modules=[
'q_proj'
,
'k_proj'
,
'v_proj'
,
'dense'
],
bias="none"
,
lora_dropout=0.05, # Conventional
task_type="CAUSAL_LM"
,
)
# 1 - Enabling gradient checkpointing to reduce memory usage during fin\
e-tuning
original_model.gradient_checkpointing_enable()
# 2 - Using the prepare_model_for_kbit_training method from PEFT
original_model = prepare_model_for_kbit_training(original_model)
Chapter 5: Fine-tuning LLMs 194
peft_model = get_peft_model(original_model, config)
Let’s go through the key components of the above code:
• LoraConfig, setting the LoRA parameters:
– r=32: The rank of the low-rank matrices in LoRA. This controls the dimen-
sionality of the factorization applied to the model weights. Higher ranks
increase model capacity, but also require more memory and computation.
– lora_alpha=32: A scaling factor for the LoRA updates. This controls how
much the low-rank matrices contribute to the overall model output. It’s a
hyperparameter that balances between the original model weights and the
LoRA adaptation.
– target_modules: The list of modules (layers) in the model where LoRA
will be applied:
* q_proj: Query projection in the transformer attention mechanism.
* k_proj: Key projection.
* v_proj: Value projection.
* dense: Dense (fully connected) layer in the transformer. These are crit-
ical parts of the configuration, and fine-tuning them can significantly
improve performance.
– “‘bias=“none”“: Specifies that biases in the layers will not be fine-tuned
with LoRA. Other options include all (fine-tune all biases) or lora_only
(fine-tune only biases in LoRA layers).
– lora_dropout=0.05: Dropout rate applied to the LoRA layers, which helps
prevent overfitting by randomly dropping out parts of the model during
training.
– task_type="CAUSAL_LM": Specifies the type of task being fine-tuned. Here,
it’s set to CAUSAL_LM (Causal Language Modeling), which is commonly
used for models like Llama, Mistral.
• original_model.gradient_checkpointing_enable() enabling Gradient
Checkpointing. This reduces memory usage during training by storing only
part of the model’s activations and recomputing them when necessary.
This trades off memory for additional computation, which is beneficial in
memory-constrained environments, especially when fine-tuning large models.
Chapter 5: Fine-tuning LLMs 195
• original_model = prepare_model_for_kbit_training(original_model) pre-
pares the model for k-bit training, such as 4-bit or 8-bit quantization which
we discussed earlier.
Once everything is set up, you can use the print_number_of_trainable_model_-
parameters function to see how many trainable model parameters exist.
print(print_number_of_trainable_model_parameters(peft_model))
Result:
trainable model parameters: 20971520
all model parameters: 1542364160
percentage of trainable model parameters: 1.36%
Part 4. Train the model
Before training the model, there’s one final step to complete: defining the training
arguments and creating a Hugging Face Trainer instance.
It’s a best practise to use parameters using TrainingArgs, which allows you to control
how the optimization process is set up. The Transformers library provides numerous
well-documented training arguments that can be used for this purpose.
Step 1: Train the model
To start, let’s configure the training arguments as follows:
Chapter 5: Fine-tuning LLMs 196
output_dir = './peft-scientific_reports-summary-training/final-checkpoi\
nt'
max_steps=400
import transformers
peft_training_args = TrainingArguments(
output_dir = output_dir,
warmup_steps=1,
per_device_train_batch_size=1,
gradient_accumulation_steps=4,
max_steps=max_steps,
learning_rate=2e-4,
optim="paged_adamw_8bit"
,
logging_steps=25,
logging_dir="./logs"
,
save_strategy="steps"
save_steps=25,
eval_strategy="steps"
,
,
eval_steps=25,
do_eval=True,
gradient_checkpointing=True,
report_to="none"
,
overwrite_output_dir = 'True'
,
group_by_length=True,
)
peft_model.config.use_cache = False
• output_dir: We specified the output directory where the the final model
checkpoint and training outputs will be saved in Colab environment.
• max_steps=400: Sets the maximum number of training steps. The training
process will terminate once 400 steps are reached, regardless of the number
of epochs. You can set this number as needed, but be sure to keep it at or above
100, since fewer than 100 steps may not allow the adapter to train adequately.
Chapter 5: Fine-tuning LLMs 197
Now, let’s focus on the key optimization techniques employed in the
TrainingArguments instance shown earlier:
• per_device_train_batch_size=1: Batch size of 1 for training on each device
(GPU). This can be small due to memory limitations, especially with large
models.
• gradient_accumulation_steps=4: Instead of updating the model’s weights
after every batch, it accumulates gradients over 4 batches and performs an
update after those 4 batches. This effectively simulates a larger batch size
without needing more memory.
• max_steps=max_steps: Specifies the maximum number of training steps (400 in
this case).
• optim="paged_adamw_8bit": Specifies the optimizer to use. AdamW is a
popular optimizer for transformer models, and the paged_adamw_8bit version
reduces memory usage by working in 8-bit precision, allowing for more efficient
training.
• save_strategy="steps": Specifies how often model checkpoints should be
saved. Here, it’s set to save the model after a certain number of steps.
• save_steps=25: Saves the model’s state every 25 training steps. You can
increase the number to 50.
• gradient_checkpointing=True: Enables gradient checkpointing to reduce
memory consumption. This trades off memory for compute, as the intermediate
activations are recomputed during backpropagation.
• report_to="none": Prevents logging to any external platform (e.g., Hugging
Face Hub). This can be set to wandb or tensorboard if you want to use a logging
platform.
We also disabled the caching for the model peft_model.config.use_cache = False.
Caching is typically useful when generating sequences, but during training, disabling
it reduces memory usage.
After completing the setup process, we’re now ready to train the model. To proceed,
add the following code and execute the cell.
Chapter 5: Fine-tuning LLMs 198
peft_trainer = transformers.Trainer(
model=peft_model,
train_dataset=train_dataset,
eval_dataset=eval_dataset,
args=peft_training_args,
data_collator=transformers.DataCollatorForLanguageModeling(tokenize\
r, mlm=False),
)
peft_trainer.train()
If you don’t adjust the max_steps configuration from 400, the training process will
take approximately an hour to complete. In the meantime, you can grab a cup of
coffee.
If everything runs smoothly, you should see the final results once the process
completes all the steps.
Chapter 5: Fine-tuning LLMs 199
Figure 5.8
Congratulations, you trained your model with a custom dataset. This table represents
the training progress of the QLoRA adapter over time, showing how both the
Training Loss and Validation Loss decrease as the training progresses through
various steps.
Note that, Training Loss decreases steadily from 2.243100 at step 25 to 0.043000 at
step 400. This indicates that the model is learning and improving its performance on
the training data as the training progresses. In the meantime Validation Loss also
Chapter 5: Fine-tuning LLMs 200
decreases, starting at 1.841917 and reaching 0.011215 by step 400. The validation loss
is slightly higher than the training loss, which is normal since the model is being
evaluated on unlabeled data.
It’s worth mentioning that when we configured our training arguments, the Hugging
Face Trainer created a checkpoint every 25 steps. Each checkpoint is stored in
a separate folder, containing eight files related to the QLoRA adapter. Every
checkpoint folder holds a complete QLoRA adapter, which can be used with the
pre-trained model. Download a few of these checkpoint folders, including all files,
to your local system for use in the next step.
Figure 5.9
Chapter 5: Fine-tuning LLMs 201
Warning
Click on the folder icon on the left side of the Colab IDE to view all the
checkpoint folders generated during the training period. These folders will
remain accessible until the Colab runtime session ends, so be sure to copy
a few checkpoints before the session concludes.
The final step is to evaluate the QLoRA adapter we generated and see how well it
performs on our summarization task, which we’ll go over by the end of this chapter.
Part 5. Evaluate the model
Imagine you’re testing how well someone learned a language (let’s say, English).
You’d give them tasks like answering questions, understanding stories, or holding a
conversation. Similarly, LLM evaluation involves giving the model different tasks to
see how well it:
• Answers Questions: Can it give accurate, useful answers?
• Understands Text: Can it grasp the meaning of a sentence or passage?
• Generates Language: Can it write coherent and relevant text?
During evaluation, experts or automated tools measure the model’s performance
using different metrics, such as accuracy, relevance, fluency, and factual correctness.
In simpler terms, it’s like grading a student’s ability to read, write, and understand
language, but here the student is a large LLM.
There are two widely used types of LLM evaluations, each offering a different way
to measure how well a model like an LLM performs:
Chapter 5: Fine-tuning LLMs 202
1. Human Evaluation. Human evaluation involves real people assessing the
performance of a language model. People judge whether the model’s output
is accurate, relevant, and natural-sounding.
Evaluators (often language experts) read the text generated by the model and rate it
based on various criteria like:
• Fluency: Does the text sound natural, like something a human would write?
• Relevance: Is the information on topic?
• Correctness: Is the output factually accurate or makes sense?
Note that, Human evaluation can be subjective, slow, and expensive because it
requires a lot of manual work and very depend on the evaluators.
2. Evaluation Metrics. Evaluation metrics are tools or methods used to measure
how well a machine learning model or system performs its tasks. They help
assess the accuracy, quality, and effectiveness of a model by comparing its
predictions or outputs against the correct or expected results.
Evaluation metrics are task-specific, meaning different tasks require different met-
rics. For example, a translation model may use the BLEU score, while a summariza-
tion model might rely on the ROUGE score or precision. However, there are several
evaluation metrics commonly used in the AI industry:
• BLEU Score: Evaluates the fluency and coherence of generated text compared
to human-written text.
• ROUGE Score: Similar to BLEU, measures the overlap between model-
generated text and reference text (commonly used in summarization).
• METEOR Score: Measures the similarity between generated and human-
written text based on surface-level features.
Let’s apply these two types of LLM evaluation in practice. To evaluate our QLoRA
adapter with the pre-trained Ph-2 model, we will use human evaluation and the
ROUGE score, which are well-suited for text summarization.
Chapter 5: Fine-tuning LLMs 203
Also, note that we will use a new Colab notebook for evaluation to avoid out-of-
memory errors. While there are a few ways to address these issues, they don’t always
work if the dataset is too large to fit into memory (especially since we only have 16GB
of GPU memory on Colab).
Most of the code snippets have already been used and explained in detail in previous
steps, so we will skip the explanations here. Download the notebook from the GitHub
repository, upload it to Colab, and execute the cells up to cell 8.
We will use Google Drive to transfer our generated QLoRA adapter to the Colab
notebook runtime. First, upload the checkpoint-400 file (or a similar file saved on
your drive) to Google Drive. Then, execute the following cell.
from google.colab import drive
drive.mount('/content/drive')
Google Colab will ask for permission to access your Google Drive. Grant permission
by selecting the appropriate checkbox, and you’re all set to go.
Info
Instead of using Google Drive, you can upload the file directly to the Colab
Files section. However, this method often results in errors due to file
corruption during upload. Therefore, we highly recommend using Google
Drive for working with files in Colab notebooks. Additionally, the QLoRA
adapter is also available for download from the GitHub repository.
After successfully connecting to Google Drive, your files will be available in the
Colab notebook as shown on figure 5.10.
Chapter 5: Fine-tuning LLMs 204
Figure 5.10
Update the file path /content/drive/MyDrive/checkpoint-400 with the location of
your file on Google Drive, and then execute the cell.
from peft import PeftModel
ft_model = PeftModel.from_pretrained(
base_model,
"/content/drive/MyDrive/checkpoint-400"
torch_dtype=torch.float16,
is_trainable=False
)
,
Add the following code snippet, or execute the cell if you’ve uploaded the notebook
to Colab from the GitHub repository.
Chapter 5: Fine-tuning LLMs 205
%%time
import pandas as pd
txt_list = dataset['test'][21:23]['text']
summaries = dataset['test'][21:23]['summary']
original_model_summaries = []
instruct_model_summaries = []
peft_model_summaries = []
for idx, text in enumerate(txt_list):
txt = txt_list[idx]
summary = summaries[idx]
prompt = f"Instruct: Summarize the following scientific reports .\n{t\
xt}\nOutput:\n"
peft_model_res = gen(ft_model, prompt, 100,)
peft_model_output = peft_model_res[0].split('Output:\n')[1]
peft_model_text_output, success, result = peft_model_output.partition\
('#End')
original_model_res = gen(base_model, prompt, 100,)
original_model_text_output = original_model_res[0].split('Output:\n')\
[1]
original_model_summaries.append(original_model_text_output)
peft_model_summaries.append(peft_model_text_output)
instruct_model_summaries.append(summary)
zipped_summaries = list(zip(instruct_model_summaries, original_model_su\
mmaries, peft_model_summaries))
df = pd.DataFrame(zipped_summaries, columns = ['baseline_summaries'
riginal_model_summaries'
,
'peft_model_summaries'])
,
'o\
Chapter 5: Fine-tuning LLMs 206
df
This code imports the Python pandas library for data manipulation and analysis. It
then retrieves a specific subset of texts and their corresponding summaries from the
dataset (rows 21 and 22). The code iterates over this subset, constructs prompt to ask
the models to summarize the texts, and uses two models (ft_model and base_model)
to generate these summaries. Next, it extracts the generated summaries from the
model outputs and appends them to a pandas DataFrame. This DataFrame includes
columns for baseline summaries, original model summaries, and PEFT model
summaries for visualization.
The output of the code execution should resemble what is displayed below:
Figure 5.11
Looks good; the trained QLoRA adapter performs better than the pre-trained model.
Let’s compare the results from the zero-shot summarization of the pre-trained model
from step 8.
Chapter 5: Fine-tuning LLMs 207
Original model summaries
(ZERO SHOT)
PEFT model summaries
MODEL GENERATION SUM-
MARY:2(kxp;2(kxp(p(1)training.
We pick a ksuch that it can
support our analysis thatSwim
outperforms
This paper presents Swim, in
benchmark tests on MuJoCo’s
continuous control tasks. even
when the best-performing Swish
function was used as introducing
a new scaling parameter to
replace the We
The results appear to be much better with the trained model.
Warning
Results may vary depending on the number of training steps used. We used
400 steps to train the model. As mentioned before, the more you train the
model, the better it becomes at completing the task.
So far, we have finished the human evaluation, let’s get started with the ROUGE
Score metrics.
Execute the following fragment of codes:
import evaluate
rouge = evaluate.load('rouge')
original_model_results = rouge.compute(
predictions=original_model_summaries,
references=original_model_summaries[0:len(original_model_summaries)\
],
use_aggregator=True,
use_stemmer=True,
)
Chapter 5: Fine-tuning LLMs 208
peft_model_results = rouge.compute(
predictions=peft_model_summaries,
references=original_model_summaries[0:len(peft_model_summaries)],
use_aggregator=True,
use_stemmer=True,
)
print('ORIGINAL MODEL:')
print(original_model_results)
print('PEFT MODEL:')
print(peft_model_results)
print("Absolute percentage improvement of PEFT MODEL over ORIGINAL MODE\
L")
improvement = (np.array(list(peft_model_results.values())) - np.array(l\
ist(original_model_results.values())))
for key, value in zip(peft_model_results.keys(), improvement):
print(f'{key}: {value*100:.2f}%')
The code above utilizes the Python evaluate library to compute ROUGE scores for
two sets of summaries: one generated by the original model and the other by the
PEFT model. It then calculates the absolute percentage improvement of the PEFT
model over the original and displays the results.
Let’s examine the result:
Chapter 5: Fine-tuning LLMs 209
ORIGINAL MODEL:
{'rouge1': 1.0,
'rouge2': 1.0,
'rougeL': 1.0,
'rougeLsum': 1.0}
PEFT MODEL:
{'rouge1': 1.0798425196850394,
'rouge2': 1.042739726027397,
.00356955380578,
'rougeLsum': 1.719685039370079}
Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL
rouge1: 0.1%
rouge2: 0.03%
rougeL: 0.01%
rougeLsum: 1.60%
'rougeL': 1\
The PEFT model shows slight improvements over the original model across all
ROUGE metrics, with the most noticeable improvement in ROUGE-Lsum. Although
the percentage changes are small, indicating only minor improvements, the differ-
ences become more significant when compared to the zero-shot summaries from step
8. You can perform this evaluation yourself as a homework assignment.
Part 6. Save & deploy the final model
Suppose you are satisfied with the performance of your QLoRA adapter and want to
deploy it for local or production inference. To do this, you need to follow a few steps,
which vary depending on the framework used to train the model. In Unsloth, the
process is straightforward and integrated into the framework. However, for Hugging
Face Trainer, you will need to follow these steps:
• Merge the adapters into the base model so you can use the model like a normal
transformers model as shown below (where ft_model is our QLoRA adapter
defined before):
model = ft_model.merge_and_unload()
model.save_pretrained("merged_adapters")
• After merging the adapters, you can proceed to run text summarization just
like with a standard model, as described earlier. Please note that the combined
model will be substantially larger than usual, approximately 2 GB in size
Chapter 5: Fine-tuning LLMs 210
• Download the final model to your hard drive from the Google Colab notebook.
You should find a folder named merged_adapters containing three files.
At this stage, you have two options for running inference with your merged model:
• Upload and Use as a Regular LLM: You can upload the merged adapter to
Hugging Face Hub and utilize it as a standard LLM, just like we did previously.
• Convert and Run Locally on Ollama: Alternatively, you need to convert the
final model into the GGUF format if you want to run it on Ollama as a local
LLM runner. Once the conversion is complete successfully, you can execute the
model in Ollama just like any other standard LLM.
The final steps involve setting up your environment-specific configurations. As
these are typically tailored to individual needs and mostly relevant for production or
research purposes, we’ve omitted the details. For more information on this process,
please visit our accompanying blog post on the website.
Conclusion
In conclusion, fine-tuning large language models is crucial for adapting pre-trained
models to specialized tasks. By leveraging advanced techniques such as full fine-
tuning, parameter-efficient fine-tuning (PEFT), LoRA, and QLoRA, developers can
optimize performance while minimizing resource usage.
Fine-tuning not only enhances task-specific capabilities but also preserves the
model’s pre-existing knowledge, enabling efficient deployments across various ap-
plications. As LLMs continue to advance, incorporating cutting-edge fine-tuning
techniques and tools into your workflow can further boost accuracy and deliver more
reliable results.