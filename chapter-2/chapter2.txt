Chapter 2: Deep dive into the
theories of Generative AI
Generative AI is a revolutionary field of artificial intelligence focused on creating
new content, from text and code to images, audio, and even videos. Unlike
traditional AI systems that analyze and interpret existing data, generative models
learn patterns and structures within vast datasets to produce original and often
surprising (!) outputs. Powered by deep learning algorithms, these models can mimic
human creativity and generate content that is indistinguishable from human-made
work. This opens up a world of possibilities, from automating creative tasks and
personalizing user experiences to accelerating scientific discovery and pushing the
boundaries of artistic expression.
What exactly is Artificial Intelligence, and Generative AI? AI is not a revolutionary
technology but rather an evolution of algorithms and technologies over the past
decade.
In our opinion, one of the main obstacles to learning Generative AI (Gen AI) is
not understanding the basic terminology and jargon. Since we’re about to explore
Generative Artificial Intelligence, let’s start by providing some context.
Artificial Intelligence (AI)
Artificial Intelligence, or AI, is a branch of computer science focused on developing
theories and methods to create applications or machines that can mimic human
behavior. Therefore, AI is a discipline, much like physics is a discipline within
science.
The key features of AI are:
1. Reasoning: AI systems should be able to process information and make logical
deductions, similar to how humans do.
Chapter 2: Deep dive into the theories of Generative AI 45
2. Learning: AI aims to enable machines to learn from data, identify patterns, and
improve their performance over time without explicit programming for every
scenario.
3. Acting: AI systems should be able to take actions in the real world based on
the information they process and their goals.
There are several distinct types of artificial intelligence:
• Narrow or Weak AI: Designed to perform specific tasks, like playing chess or
recommending products.
• General or Strong AI: Hypothetical AI with human-level intelligence and the
ability to perform any intellectual task that a human can.
• Super AI: Hypothetical AI that surpasses human intelligence in all aspects.
AI is rapidly evolving and impacting various fields, including healthcare, finance
tech, transportation, and entertainment.
Figure 2.1
As mentioned earlier, AI is a discipline rather than a natural science and consists
of several subfields, as illustrated in Figure 2.1. You have likely come across similar
images on the internet, as Figure 2.1 is a common illustration used to explain AI
Chapter 2: Deep dive into the theories of Generative AI 46
and its subfields. fundamentally begins here.
The first subset of the AI discipline is machine learning, AI
Machine Learning (ML)
Machine learning is a subfield of Artificial Intelligence (AI) that enables computers
to learn from data without being explicitly programmed.
If you have ever shopped on Amazon or eBay, you have likely interacted with a
machine learning model. Recommendation systems, a type of machine learning
model, are commonly used on e-commerce platforms like Amazon to suggest books
or products based on users past behavior or preferences. Let’s explore an example to
clarify the concept of machine learning.
Imagine we have the following dataset on our e-commerce site.
User ID Book Title Genre Author Rating (1-5)
1 The Apache
Ignite book
Technology Shamim
Ahmed
5
1 Getting
started with
generative AI
Technology Shamim
Ahmed
5
2 High
performance
in-memory
computing
Technology Shamim
Ahmed
4
3 The Martian Sci-Fi Andy Weir 4
3 Dune Sci-Fi Frank
Herbert
5
We want to predict and recommend a book to a user based on factors such as genre
preferences (e.g., Technology, Non-fiction, Sci-Fi), previous ratings (books the user
has rated highly), book popularity (average rating from other users), and reading
history (types of books the user frequently reads).
Chapter 2: Deep dive into the theories of Generative AI 47
Machine learning models can be used to recommend books by identifying patterns in
the behavior of multiple users. For example, if both User A and User B have highly
rated certain books, the system might suggest books that User A liked but User B
hasn’t read yet, and vice versa.
Info
The columns Genre and Rating (1-5) from the dataset are called features
in machine learning terms. These features are used by algorithms to make
predictions or classifications.
Here’s the basic idea of ML:
1. Data: Machine learning algorithms need lots of data to learn from. This data
can be anything: text, numbers, sensor readings, etc.
2. Algorithm: This is the learning recipe that the computer uses to analyze the
data and find patterns. There are many different types of algorithms, each
suited for different tasks.
3. Training: The algorithm is fed the data and adjusts its internal parameters to
improve its performance on a specific task. Think of it like the child learning
from the cat pictures – they gradually get better at recognizing cats.
4. Prediction: Once trained, the algorithm can make predictions on new, unseen
data. For example, it could recommend a book that a user hasn’t read yet.
This idea can be illustrated as a machine learning process, as shown in Figure 2.2.
Figure 2.2
Chapter 2: Deep dive into the theories of Generative AI 48
Once the ML engineer finished training and optimized the Algorithm, it will follow
the standard process:
1. Recieve a new input as a User query.
2. Analyze the data.
3. Find a pattern.
4. Make a prediction, recomendation or classification.
5. Send a result back to the User.
There are a few types of ML:
• Supervised Learning: The algorithm is trained on labeled data (e.g., data from
database tables). It learns to map input data to the correct output label.
• Unsupervised Learning: The algorithm is trained on unlabeled data (e.g.,
images of dog, cat etc.) and must discover patterns and relationships on its
own.
• Reinforcement Learning: The algorithm learns through trial and error, receiv-
ing rewards for correct actions and penalties for incorrect ones.
Examples of Machine Learning in action is as follows:
• Recommendation systems: Netflix suggesting movies you might like or
Amazon product recommendations.
• Spam filters: Identifying and blocking unwanted emails.
• Fraud detection: Flagging suspicious transactions on banking system.
Machine learning is revolutionizing many industries and aspects of our lives, and
its potential continues to grow. If you’re interested in machine learning and want to
learn more, we highly recommend picking up a book on the topic and experimenting
with some frameworks to get hands-on experience.
Chapter 2: Deep dive into the theories of Generative AI 49
Deep Learning (DL)
Deep learning is a subfield of Machine Learning or can be said as a special kind
of ML. It works technically in the same way as machine learning does, but with
different capabilities and approaches. DL uses Artificial Neural Networks (ANNs) to
learn patterns and relationships from data.
The key difference is, machine learning algorithms learn from data by identifying
simple patterns and relationships when Deep learning uses artificial neural networks
with multiple layers (hence deep) of interconnected nodes to learn complex patterns
and representations. These layers allow the algorithm to gradually build a deeper
understanding of the data, similar to how our brain processes information.
Info
Artificial Neural Network (ANN) in plain English: Imagine your brain has
billions of tiny, specialized computing units called neurons (or nodes) that
communicate with each other through connections (like tiny synapses). An
ANN is a simplified model of this process, using artificial nodes and layers
to make predictions or classify data. Each layer analyzes and transforms
the input data in some way, allowing the network to learn more complex
patterns and relationships.
The diagram shown as Figure 2.3 displays a basic structure of an Artificial Neural
Network, where it consists of multiple layers called the input layer, output layer,
and hidden layers. In each layer every node (neuron) is connected to all nodes
(neurons) in the next layer with parameters called weights.
Chapter 2: Deep dive into the theories of Generative AI 50
Figure 2.3
Let’s consider the example, you’re trying to recognize a Bicycle picture. A simple
computer program might look for basic features like Handlebar or Wheels. But with
Deep Learning, we create an ANN that looks at multiple layers of detail:
• Layer 1: Basic shapes (e.g., circles, lines)
• Layer 2: Object parts (e.g., Handlebar, Wheels)
• Layer 3: Overall object recognition (e.g., Bicycle)
Chapter 2: Deep dive into the theories of Generative AI 51
Figure 2.4
Each layer builds upon the previous one, allowing the network to learn more complex
and nuanced features of the image. This process is called training, where we feed
the network with many examples of Bicycle pictures and adjust the connections
between nodes based on the errors or misclassifications.
Info
Please note that in the context of Deep Learning, the following terms will
be used extensively throughout this book: Nodes, Layers, Weights. These
terms are fundamental to understanding various concepts, including fine-
tuning large language models and image processing.
Deep learning uses the following features:
• Feature extraction: Deep learning algorithms can automatically learn relevant
features from raw data, eliminating the need for manual feature engineering.
• Handling complexity: Deep networks can handle vast amounts of complex
data, making them suitable for tasks like image recognition, natural language
processing, and speech recognition.
• Generalization: Deep learning models tend to generalize well to new data,
meaning they can perform well on unseen examples.
Chapter 2: Deep dive into the theories of Generative AI 52
Examples of Deep Learning in Real-World Scenarios:
• Image recognition: Identifying objects, faces, and scenes in images.
• Speech recognition: Converting spoken words into text.
• Self-driving cars: Perceiving the environment and making driving decisions.
Deep Learning models can be initially trained on vast amounts of data, and then
tailored to perform specific tasks or operate within particular domains. This capabil-
ity to process massive datasets and learn from raw text has made Deep Learning
an essential component of cutting-edge applications such as Natural Language
Processing (NLP).
Natural Language Processing (NLP)
NLP is a subfield of AI (this is where the confusion begins!!) that focuses on enabling
computers to understand, interpret, and generate human language in a way that
is both meaningful and useful. It combines computational linguistics, computer
science, and data science to bridge the gap between human communication and
machine understanding.
NLP and deep learning are closely related, with deep learning providing powerful
tools and techniques that have significantly advanced the capabilities of NLP. Here’s
how they are related:
• Neural Networks: Deep learning employs neural networks with multiple
layers (deep neural networks) to model complex patterns in data. In NLP, these
models are used to process and understand text by learning from large corpora
of language data.
• Recurrent Neural Networks (RNNs): Early deep learning approaches in NLP
used RNNs to handle sequential data and capture dependencies in text. Variants
like Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRUs) were
introduced to address limitations in handling long-range dependencies.
• Convolutional Neural Networks (CNNs): Though traditionally used for
image processing, CNNs are also applied to NLP for tasks like sentence
classification and named entity recognition by capturing local patterns in text.
Chapter 2: Deep dive into the theories of Generative AI 53
Info
NLP is not a subset of deep learning. Instead, deep neural networks,
particularly deep learning models, are used to enhance NLP capabilities
by learning complex patterns and representations from large datasets.
NLP can be broadly divided into two areas: Natural Language Understanding
(NLU) and Natural Language Generation (NLG). NLU focuses on helping ma-
chines understand and analyze human language by identifying concepts, entities,
emotions, and keywords. NLG, on the other hand, involves creating coherent and
meaningful text, such as phrases, sentences, and paragraphs, from internal data
representations.
Models designed to understand and generate human language are commonly re-
ferred to as Language Models (LMs). Typically, these models are based on recurrent
neural networks (RNNs), which have proven effective at capturing long-range
dependencies within sequential data, such as text. However, RNN-LMs can be
computationally intensive to train and susceptible to overfitting, especially when
dealing with longer sequences of words
The rise of deep learning models in 2016, especially Transformers, has revolution-
ized both NLU and NLG, leading to significant advancements in natural language
understanding and generation, which have greatly improved the accuracy and
effectiveness of NLP applications.
We will now smoothly transition to explore the Transformer model in detail,
examining its structure and how it works. For further reading on NLP concepts,
you can find more comprehensive information here.
Transformer
The Transformer model was first introduced in the ground-breaking paper Attention
Is All You Need, which presented a revolutionary new approach to deep learning
neural networks that has had a profound impact on Natural Language Processing
(NLP). Unlike traditional models such as Recurrent Neural Networks (RNNs) and
Chapter 2: Deep dive into the theories of Generative AI 54
Convolutional Neural Networks (CNNs), Transformer models utilize self-attention
mechanisms to more effectively capture the complex relationships between words
within a sentence. This ability to handle long-range dependencies, regardless of their
distance in the text, has made Transformers highly effective for a wide range of NLP
tasks.
Key components (there are much more in the original paper) of Transformer models
include:
• Self-Attention Mechanism: This enables the model to assess the importance
of different words in a sentence relative to each other, capturing complex
relationships.
• Encoder-Decoder Architecture: Essential for tasks like machine translation,
where the encoder processes the input text, and the decoder generates the
output text.
In the following sections, we will explore each of these components in detail.
Self-Attention mechanism
The self-attention mechanism is a key part of how Transformer models understand
language. It helps the model figure out which words in a sentence are most
important and how they relate to each other.
Imagine you are reading a sentence like The dog barked loudly because it saw a
stranger. When you read the word it, you automatically know that it refers to the
dog because of the context. The self-attention mechanism does something similar—it
looks at all the words in the sentence and focuses on the ones that are important for
understanding each other.
Let’s try to understand how it works with this above example sentence: The dog
barked loudly because it saw a stranger.
• Looking at Each Word: The self-attention mechanism looks at every word in
the sentence. When it gets to the word it, it has to figure out what it refers to.
Is it the dog? The barking? The stranger?
Chapter 2: Deep dive into the theories of Generative AI 55
• Focusing on the Important Words: The mechanism looks at the surrounding
words like dog and saw to figure out that it refers to the dog. It also focuses less
on less important words like loudly or because that don’t help in understanding
the main point.
• Assigning Importance: It assigns a weight or score to each word, giving more
attention to the words that matter. In this case, dog would get a higher score
because it’s more relevant to the meaning of it.
• Using the Connections: The model uses these connections to understand the
sentence as a whole. Now it knows that the dog is the one barking because it
saw a stranger, making the sentence clear.
Before the self-attention mechanism, older models would look at words one by one,
often losing track of important relationships between them. Self-attention allows
the model to understand the meaning of a word based on all the other words in the
sentence, making it much smarter at tasks like translation, summarizing, or even
completing your sentences.
The self-attention mechanism is widely used in everyday applications such as
Google’s Gmail or search functions. For instance, consider this scenario: when you
type into Google Search I need a hotel that is close to the airport, the
self-attention mechanism recognizes that close refers to the location of the hotel
and airport indicates its proximity to the desired destination. By understanding the
entire context, rather than just individual words, it provides more accurate search
results.
Encoder-Decoder architecture
The encoder-decoder architecture is a mechanism used by many language models,
especially in tasks like translation. It works like a two-step process where one part
the encoder takes the input information, and the other part the decoder uses that
information to produce an output.
Imagine it like a translator who first reads and understands a sentence in one
language (the encoding part) and then translates it into another language (the
decoding part).
The encoder-decoder mechanism is crucial part of any models because it allows
models to handle complex tasks like language translation, summarization, and even
Chapter 2: Deep dive into the theories of Generative AI 56
chatbots. The encoder helps the model understand the input (whether it’s a sentence,
paragraph, or image), and the decoder generates the corresponding output, ensuring
that the meaning stays accurate.
To fully understand how Transformer models process information, let’s break down
their architecture into its two primary components: the encoder and the decoder.
We’ll explore each of these crucial elements in greater detail to gain insight into
their roles and functionality.
• Encoder: Understanding the Input.
– The encoder’s job is to take the input, like a sentence, and understand its
meaning. For example, if the input sentence is The cat is sleeping on
the sofa, the encoder processes each word, understands the relationships
between the words, and converts the whole sentence into a kind of code
or summary (called a hidden representation).
– This summary captures the essence of the sentence.
• Decoder: Producing the Output.
– The decoder’s job is to take the summary from the encoder and use it to
generate the desired output. For instance, if we want to translate The cat
is sleeping on the sofa into French, the decoder will use the encoded
summary and generate the French sentence: Le chat dort sur le canapé.
– The decoder starts with the encoded summary and predicts one word at a
time, checking back with the encoder to make sure it’s on track.
We can illustrate the process as shown in figure 2.6.
Chapter 2: Deep dive into the theories of Generative AI 57
Figure 2.6
Let’s try another example, imagine the encoder-decoder mechanism as a travel guide
who helps you translate signs at an airport.
• Encoder: The travel guide reads a sign in French that says, “Sortie de secours”
(which means “emergency exit”). They understand what the sign means in
French.
• Decoder: After understanding it, the guide then tells you, “This sign means
‘emergency exit’ in English.”
The encoder understood the French sentence, and the decoder turned that under-
standing into an English sentence.
This brief overview of the encoder-decoder architecture lays a solid foundation for
understanding its core principles. In future sections of this chapter, we will revisit
the transformer architecture in more detail once we have a clearer understanding of
tokens and vectors.
The Transformer architecture serves as the foundation for the development of a
new generation of language models (LMs), which have been trained on a certain
corpus of data. Notably, models like BERT and CamemBERT are exemplary of this
new breed of LMs. These LMs are often referred to as Gen1 (Generation 1) LMs or
Language Models after Transformers. A key characteristic of these models is their
relatively small parameter count, making them well-suited for basic language tasks.
For example, BERT, developed by Google in 2018, is an encoder-only LLM that boasts
Chapter 2: Deep dive into the theories of Generative AI 58
a base variant with 12 layers and 12 attention heads, providing approximately 110
million parameters.
Figure 2.7
However, the capabilities of these transformer-based models have led to the cre-
ation of more sophisticated pre-trained models like GPT (Generative Pre-trained
Transformer) or T5. These cutting-edge models are trained on enormous amounts
of text data and can be tailored for specific NLP tasks, resulting in state-of-the-art
performance. The emergence of these advanced models has given rise to the exciting
field of generative AI.
Generative AI
Generative AI is a type of artificial intelligence developed over decades of research
in various AI fields. It combines the power of machine learning, deep learning,
and large-scale models to create new and creative content across different domains.
Unlike systems that simply recognize patterns or make predictions, generative AI
can generate entirely new content—such as text, images, music, or even code—by
synthesizing information it has learned from existing data.
Large Language Models (LLMs), such as GPT and Llama, are a prominent example of
generative AI, specializing in generating and understanding text. These models rely
on deep learning to learn patterns and representations from vast amounts of existing
data. As a result, generative AI is often considered a subset of deep learning.
Chapter 2: Deep dive into the theories of Generative AI 59
Info
Generative AI is a broad term that covers any AI that can generate new
content, whether it’s images, music, code, or text. On the other hand, LLMs
are a subset of generative AI that specifically deal with generating and
understanding human language. All LLMs are generative AI, but not all
generative AI are LLMs.
Figure 2.8
Summary of the key milestones from figure 2.8:
• Machine learning paved the way for pattern recognition.
• Deep learning and neural networks allowed models to process complex data.
• The Transformer architecture revolutionized NLP and text generation.
• Large-scale training made generative AI models like GPT or Llama powerful
enough to handle complex tasks like writing and conversation.
Chapter 2: Deep dive into the theories of Generative AI 60
As we mentioned earlier, in 2017, a significant breakthrough was made in the field of
natural language processing with the development of transformers. This innovation
allowed models to process large amounts of text much more efficiently by focusing
on the most relevant parts, using a technique called self-attention. As a result,
transformers became the foundation upon which many modern generative AI models
were built.
With the rise of massive datasets and increased computational power, researchers
began training generative models on vast amounts of data. Pretrained LLMs, like
OpenAI’s GPT and Google’s BERT are trained on massive text corpora, allowing
them to generate meaningful and coherent text or engage in conversations, as seen
in AI-powered chatbots.
Generative AI offers a wealth of benefits, and its applications are vast and varied.
Some of the most significant advantages include:
• Creativity and Innovation: Generative AI can assist in generating creative
content, such as art, music, and literature. It helps artists, writers, and musicians
explore new ideas and enhance their creative processes.
• Efficiency and Automation: It automates routine tasks and processes, which
can save time and reduce human error. For example, it can generate reports,
create marketing materials, and handle customer service inquiries.
• Personalization: Generative AI can tailor experiences and content to indi-
vidual preferences, improving user satisfaction. This is seen in personalized
recommendations on streaming services, customized advertising, and adaptive
learning systems.
• Content Creation: It helps in creating high-quality content quickly and at
scale. For instance, it can generate realistic images, write articles, or produce
videos, which is valuable for content-driven industries.
• Data Augmentation: Generative AI can create synthetic data to augment real
datasets, which is useful for training machine learning models, especially in
cases where real data is scarce or sensitive.
• Improving Accessibility: It can help create tools and applications that im-
prove accessibility, such as generating text descriptions for images or creating
voiceovers for people with visual impairments.
• Exploration and Simulation: Generative models can simulate various sce-
narios and environments, aiding in scientific research, virtual training, and
Chapter 2: Deep dive into the theories of Generative AI scenario planning.
61
What is Generative AI and what is not?
Everything in this field is evolving rapidly, and it’s easy for people to become
confused by the terms, jargon, and distinctions—especially when it comes to under-
standing what generative AI is and what it is not. In this section, we aim to clarify
the concept: What is Generative AI, and what is not?
The figure 9 shows a simple way to distinguish between what is Gen AI and what’s
not.
Figure 2.9
If a system—whether it’s machine learning, deep learning, or any other AI—produces
outputs such as numbers, classifications (e.g., “spam” or “not spam”), or probabilities
(e.g., “90% chance of buying a product”), it is not considered generative AI.
However, if the output is something like natural language (e.g., an essay or poem) or
images (e.g., pictures of dogs or cats), then it falls under the category of generative
AI.
All of the explanations above can also be expressed in mathematical terms as follows:
Chapter 2: Deep dive into the theories of Generative AI 62
Figure 2.10
The equation y = f(x) represents how the output of a process (Y) is calculated based
on different inputs (X). In this case, Y is the model’s output, F represents the function
or model used for the calculation, and X stands for the input data, which could be
anything from raw text (e.g., Wikipedia articles), CSV files (e.g., from annual reports),
or even images. The model’s output is determined by all the inputs. If Y is a number,
such as a predicted stock price, it is not considered generative AI. However, if Y is
a sentence, such as the share price, the model is performing generative AI by
generating a response.
Categories of Generative AI
In the last few years, the landscape of generative AI (Gen AI) has evolved signifi-
cantly. In 2024, we see a wide range of generative AI (Gen AI) models emerging,
including:
1. Text-to-text models: These models take text as input and generate text as
output. The text can range from natural language and programming code to
poems and even HTML. Examples include Google’s Gemini, GPT-4, Claude
Opus, and LLaMA 3.1.
Chapter 2: Deep dive into the theories of Generative AI 63
2. Text-to-image models: These models generate images based on a text descrip-
tion. For example, you could ask for an image of a dog or cat with green ears,
and the model would generate that image for you. Midjourney is an example
of such a model.
3. Image-to-image models: These models transform or combine images that you
provide as input.
4. Image-to-text models: These models extract text from images, which is particu-
larly useful for tasks like pulling text from presentations. LLaVA is an example
of this type of model.
5. Speech-to-text models: These models transcribe speech into text, which is
useful for things like meeting notes, such as those from a Zoom call. Examples
include Whisper and DeepSpeech.
6. Text-to-audio models: These models generate music or sound from a text
prompt. They are especially useful for creating audio from text, such as audio
transcriptions. Deepgram’s Aura is an example of this type of model.
7. Text-to-video models: These models generate video from a text prompt. In the
near future, we might see entire films being created from scripts. Examples
include Sora, Dream Machine, and Kling.
Generative AI applications or models can be broadly classified into two primary
categories, based on their implementation approaches:
1. Foundation models are large, pre-trained machine learning models that serve
as the base for various AI applications. These models are trained on vast, diverse
datasets (such as text, images, or audio) and can be fine-tuned for specific tasks
with minimal additional training. Due to their scale and versatility, foundation
models can be adapted to solve a wide range of tasks, from language translation
to image recognition.
As an example, DALL-E, DeepMind’s Gato is a foundation model trained on images
and text, and it generates original images based on textual descriptions.
Key characteristics:
• foundation models are trained on extensive datasets from diverse sources,
allowing them to learn a broad understanding of language, images, or other
types of data.
Chapter 2: Deep dive into the theories of Generative AI 64
• can be adapted or fine-tuned for many downstream tasks such as sentiment
analysis, text generation or object recognaization.
• their knowledge can be transferred to new tasks with minimal extra training.
2. Large Language Models (LLMs) are a type of foundation model that leverages
the power of deep learning to analyze and learn from vast amounts of data.
By consuming and training on these massive datasets, LLMs become incredibly
proficient in processing and generating human-like language. In fact, they can
create entirely new text combinations that mirror the nuances and complexities
of natural language, all based on the patterns and structures they’ve learned
from their training data.
GPT, LLaMA is the best examples of LLMs.
Key characteristics:
• designed specifically to handle natural language tasks.
• generate text, process language, and understand human language patterns.
• also trained on massive datasets from various sources.
To put it simply, LLMs are a specific type of foundation model. Both foundation
models and LLMs drive Generative AI, allowing machines к systems to create diverse
and innovative content.
If you’re feeling confused, don’t worry - it’s not your fault! The term foundation
model was introduced by researchers at Stanford’s university and the Center for
Research on Foundation Models (CRFM), but the original paper isn’t entirely clear.
What we will try to do here is break down the concept in simpler terms:
• A foundation model refers to a type of AI system that has broad capabilities,
making it adaptable to a range of different tasks and applications. Think of it
as a base layer - the original model provides a solid foundation on which other
models can be built upon. This stands in contrast to many other AI systems,
which are specifically trained for a single purpose and aren’t easily transferable
to other areas.
Chapter 2: Deep dive into the theories of Generative AI 65
• To illustrate this point, imagine a building. A foundation model would be
like the concrete slab that provides a stable base for the entire structure. You
can then build different floors, walls, and features on top of it, each serving a
specific purpose. In contrast, many other AI systems are like individual rooms
within the building - they’re designed to serve a single function and aren’t easily
modified or repurposed
Examples of foundation models often include many of the systems mentioned earlier
as LLMs. To understand how something more specific can be built on a broader base,
consider ChatGPT. Initially, ChatGPT was based on the LLM GPT-3.5, which served as
the foundation model. To tailor it for chat, OpenAI used additional data specific to
conversational settings to create a modified version of GPT-3.5. This adjusted model
was then used to develop ChatGPT.
Info
One notable example of a foundation model is DeepMind’s Gato model,
which has been designed to tackle a diverse range of tasks that extend
beyond language processing. In addition to generating text, Gato can also
control robot arms and play complex video games. This versatility makes
Gato a prime candidate for the label foundation model, but it would not
be classified as a Large Language Model (LLM).
Chapter 2: Deep dive into the theories of Generative AI 66
Figure 2.11
Currently, the term foundation model is often used interchangeably with large
language model because language models are the most prominent example of
systems with broad capabilities that can be adapted for various purposes. The key
difference between the two terms is that large language models specifically refers
to systems focused on language tasks, while foundation model represents a broader
concept that could include new types of systems in the future.
In the following sections, we will explore the different concepts behind LLMs and
explain how they operate.
Chapter 2: Deep dive into the theories of Generative AI 67
Large Language Model
So, a Large Language Model (LLM) refers to a type of model designed to perform
tasks related to language. But, why it is called Large? The answer is simple, the
term Large here refers to the number of parameters in the model (around billions)
and the amount of training data (billions to trillions of words).
Why do the model size and data volume matter so much?
When a large language model (LLM) is trained on vast datasets, it uses the input
to predict the correct output. During training, the model compares its prediction
with the actual text to see if it was right or wrong. If it makes a mistake, it adjusts
its parameters to improve. This process repeats millions or even billions of times,
allowing the model to refine its predictions and improve its accuracy. Over time, the
model becomes capable of understanding complex grammar, generating coherent
text, and following logical patterns.
How LLM works internally?
We’ve often discussed that LLMs are designed specifically for natural language
processing (NLP) tasks. But how do they understand the grammatical structure
(syntax) and meaning (semantics) of the input text, and generate an output that has
the correct syntax and relevant meaning?
In this section, we’ll explore the various techniques and mechanisms involved in NLP
processing, so get ready!
Large Language Models (LLMs) work by processing vast amounts of text data to learn
patterns in language. Internally, they rely on neural networks, which are designed
to handle sequential data efficiently.
Here’s a simplified workflow:
1. A neural network is made up of numerous numbers, or parameters, that are
interconnected. Each of these connections is called a weight.
2. The neural network only processes numbers, so when you send an input (a
prompt or question), it converts it into numbers. Based on how the parameters
are configured, the network outputs a result, also in numerical form.
Chapter 2: Deep dive into the theories of Generative AI 68
3. Any type of content—text, images, or otherwise—can be represented as numbers
and fed into the model.
Figure 2.10
For example, if we input the sentence The sky is into an LLM, it converts the words
into numbers, processes them through the neural network, and then outputs a result
like The sky is blue. This is a basic demonstration of how an LLM predicts the next
word, a common task in machine learning.
Tips
When someone says that an LLM is a collection of pretrained and fine-
tuned text models with sizes ranging from 8 billion to 405 billion param-
eters, they’re referring to the number of weights (connections between
nodes) in the model’s neural network. In Figure 2.10, our network has 18
parameters.
Now, here’s where things get really interesting: when you combine the input and
output, and feed it back to the LLM, it can continue generating new content by
predicting the next word. This process can be repeated indefinitely, allowing us
to create a continuous stream of generated text - much like what happens when you
engage with ChatGPT or LLaMA.
Chapter 2: Deep dive into the theories of Generative AI 69
At first glance, it seems simple and straightforward, doesn’t it? But beneath the
surface, there’s a lot of fascinating activity going on. Let’s break it down step by
step.
1. Tokenization: The model breaks down text into smaller units, called tokens
(words or subwords), which serve as input.
2. Embedding: Each token is converted into a numerical representation (vector)
that captures its meaning in the context of the text.
3. Transformer Architecture: The transformer consists of layers of attention
mechanisms that help the model understand the relationships between tokens,
regardless of their position in a sentence. It helps the model focus on relevant
words when generating or analyzing text.
Let’s start by the tokenization.
Tokenization
This process involves converting natural language (such as sentences) into bits of
data that a program can work with. In simpler terms, it breaks sentences down into
Chapter 2: Deep dive into the theories of Generative AI 70
individual words or parts, known as tokens, which serve as the building blocks for
semantic analysis.
Thus, A token is a unit of text segmented for efficient processing by a large language
model. LLM tries to understand the statistical relationships between these tokens for
producing the next token in a sequence of tokens.
Tokens can be entire words, letters, combinations of words, or even punctuation
marks. A tokenizer is an algorithm or function that performs this segmentation.
Figure 2.11
There are various tokenizers, each with its own trade-offs. Some well-known ones
include NLTK (Natural Language Toolkit), SpaCy, the BERT tokenizer, and Keras.
A simple word-based tokenizer would treat each word as a token, meaning the
number of tokens would match the number of words in the sentence. For example,
the above sentence would have 15 tokens. Below is an example of how the SpaCy
tokenizer splits a sentence into tokens.
import spacy
nlp = spacy.blank("en")
tokens = nlp ("We must always change, renew, rejuvenate ourselves; othe\
rwise, we harden.")
len (tokens)
for token in tokens:
print (token)
If you execute the above code, the output should same as shown below:
Chapter 2: Deep dive into the theories of Generative AI 71
We
must
always
change
,
renew
,
rejuvenate
ourselves
;
otherwise
,
we
harden
.
In tokenization, characters are often counted individually, including spaces between
words. However, not all tokenizers operate this way, some consider the context in
which the text is being used, resulting in varying outcomes.
Google’s LLM, BERT, employs a context-dependent tokenizer to achieve its language
understanding capabilities. For comparison, using the same tokenizer as GPT-4 (and
available online through OpenAI’s free online token counter), we can see how the
previous sentence is broken down into distinct 17 tokens.
Figure 2.15
Tokens are also a useful unit of measurement. The amount of text an LLM can process
or generate is measured in tokens. Additionally, the cost of running LLMs is directly
tied to the number of tokens processed, the fewer the tokens, the lower the cost, and
vice versa.
The size of an LLM is defined by the number of tokens it can accept as input, and each
Chapter 2: Deep dive into the theories of Generative AI 72
model has different limitations. Since tokens are stored and processed in memory,
these limits help keep the model efficient and optimize resource use. Below are some
of the token limitations for different LLMs:
Model Tokens Limit
Llama 2 4096
GPT 4 8192
After the input text is split into tokens, the tokenizer encodes the input using a specific
scheme and generates specialized vectors that the LLM can understand. In the next
section, we’ll explore vectors and how tokens are represented in vector format.
Vector
As we have split the input, LLMs still struggle to understand the meaning of the text.
Therefore, our first step should be to develop a strategy for converting strings into
numerical representations (or vectorizing the text) before feeding it to the model.
Vectors play a critical role in the functionality of LLMs and overall Generative
AI. This is because LLMs can only process numbers in a specific format. To fully
appreciate the role and value of vectors in LLMs, it’s essential to understand what
vectors are, how they’re generated, and how they’re utilized in these models.
In mathematics, a vector is an object that represents both the value and direction
of a quantity in any dimension. Consider the following example, where a vector
represents five elements stored in an array.
# Creating a vector as an array
!pip install numpy
import numpy as np
vector = np.array([1, 2, 3, 4, 5])
print("Vector of 5 elements:", vector)
Output:
Chapter 2: Deep dive into the theories of Generative AI 73
Vector of 5 elements: [1 2 3 4 5]
In the context of LLMs, vectors are used to represent text or data in numerical format,
allowing the model to understand and process it. This is necessary because machines
only understand numbers, so text and images must be converted into vectors for the
LLM to comprehend.
Since neural networks and transformer architectures can only understand vectors,
this representation is essential for successful processing. Moreover, operations on
vectors, enable us to determine whether two vectors are identical or distinct. At its
core, this capability forms the foundation for similarity search in vector databases or
memory storage.
Tips
This is where the new trend of Vector Databases comes in. Vector databases
store data as high-dimensional vectors, called embeddings, which capture
semantic meaning and relationships. They use specialized indexing tech-
niques, such as hashing, quantization, and graph-based methods, to enable
fast querying and similarity or semantic searches. LLMs do not directly use
vector databases; however, they are essential when you plan to enhance an
LLM with Retrieval-Augmented Generation (RAG), as discussed in Chapter
3.
When text passes through a tokenizer, it split and encodes the input according to
a specific scheme and generates specialized vectors that can be understood by the
LLM. Notably, the encoding scheme is highly dependent on the specific LLM being
used. The tokenizer may choose to convert each word and part of a word into a
vector based on this encoding scheme. Conversely, when a token passes through a
decoder, it can be easily translated back into text.
The following is pseudocode that converts our motivational text into tokens using
the Phi-2 model. We use the AutoTokenizer class from Hugging Face to encode the
text into vectors and decode it back into text.
Chapter 2: Deep dive into the theories of Generative AI 74
!pip install transformers
from transformers import AutoTokenizer
from huggingface_hub import interpreter_login
# Use your API KEY of Hugging Face here and click enter ;-)
interpreter_login()
model_name='microsoft/phi-2'
tokenizer = AutoTokenizer.from_pretrained(model_name,token="HF_TOKEN")
txt = "We must always change, renew, rejuvenate ourselves; otherwise, w\
e harden."
token = tokenizer.encode(txt)
print(token)
decoded_text = tokenizer.decode(token)
print(decoded_text)
Output:
[1135, 1276, 1464, 1487, 11, 6931, 11, 46834, 378, 6731, 26, 4306, 11, \
356, 1327, 268, 13]
We must always change, renew, rejuvenate ourselves; otherwise, we harde\
n.
Tokenization involves various techniques in practice, such as padding, end-of-line
markers, and more. The details of tokenization are covered in Chapter 5 (Fine-tuning
LLMs).
Vectors alone are not sufficient for LLMs because they only capture basic numerical
features of a token, without encoding its rich semantic meaning. Vectors are simply a
mathematical representation that can be fed into the model. To capture the semantic
relationships between tokens, we need something more—embeddings.
Chapter 2: Deep dive into the theories of Generative AI 75
Embedding
An embedding is a more sophisticated version of a vector, usually generated through
training on large datasets. Unlike raw vectors, embeddings capture semantic
relationships between tokens. This means that tokens with similar meanings will
have similar embeddings, even if they appear in different contexts.
Embeddings are what enable Large LLMs to grasp the subtleties of language,
including context, nuance, and the meanings of words and phrases. They arise from
the model’s learning process, as it absorbs vast amounts of text data and encodes not
just the identity of individual tokens but also their relationships with other tokens.
Through embeddings, LLMs gain a profound understanding of language, empow-
ering them to perform tasks such as sentiment analysis, text summarization, and
question answering with precision and nuance. Embeddings serve as the entry point
for the model, allowing it to access and process vast amounts of text data. Moreover,
they are also used independently to transform text into vectors while preserving its
semantic context.
When text is fed through an embedding model, a vector is generated that encapsu-
lates the embeddings. This process retains the essential meaning and relationships
between words and phrases, making it possible to analyze, summarize, or answer
questions based on the original text.
Typically, embeddings are generated through techniques such as Word2Vec, GloVe, or
using modern neural networks like transformers. Here’s an example of how OpenAI
Embeddings can be used to generate embeddings from input texts: Lion, Tiger and
IPhone.
from langchain_openai import OpenAIEmbeddings
embeddings_model = OpenAIEmbeddings(api_key="YOUR OPEN API KEY")
txt ="Lion"
embedded_query = embeddings_model.embed_query(txt)
print (len(embedded_query))
embedded_query[:5]
Chapter 2: Deep dive into the theories of Generative AI 76
Output:
Embedding: Lion Embedding: Tiger Embedding: Iphone
[-0.0015009930357336998,
[-0.013500549830496311,
[-0.0076760281808674335,
-0.010024921968579292,
-0.009651594795286655,
-0.021282033994793892,
-0.015631644055247307,
-0.008970077149569988,
0.0060024564154446125,
-0.023150335997343063,
-0.0019277227111160755,
-0.02365402691066265,
-0.01021870318800211]
-0.018589219078421593]
-0.016182253137230873]
Notice how the embeddings for Lion and Tiger are more similar to each other
compared to IPhone. This is because embeddings capture relationships like gender,
royalty, or object categories, and represent these concepts in a continuous vector
space.
Before we dive into the more technical details of LLMs, such as the Transformer
architecture, let’s quickly review what we’ve covered so far.
1. Tokenization: The process of converting raw text into tokens (words or
subwords). Example: "Machine learning" ￿ ["Machine", "learning"].
2. Vector & Vectorization: Each token is mapped to a numerical vector. Example:
"Machine" ￿ [0.5, 0.1, 0.7,...].
3. Embedding: A learned numerical representation that captures semantic mean-
ing. Example: Machine and Learning would have similar embeddings if the
model understands that they are related terms.
Transformers
As we learned earlier, the core mechanism of LLMs is the Transformer architecture,
particularly the attention mechanism, which allows models to handle tasks like
translation and summarization. It helps models focus on important parts of the
input, such as specific words in a sentence, when making predictions or interpreting
meaning.
Let’s break down how it works, in simple terms.
Chapter 2: Deep dive into the theories of Generative AI 77
Imagine you’re reading a long sentence. As humans, we naturally understand which
parts of a sentence are most important to focus on depending on the context. For
example:
• Sentence: The ocean is blue because it has no color itself.
When you read this sentence, you naturally understand that it refers to the ocean.
Your brain knows that it is connected to the ocean and not blue or color. But for
a machine, it’s not that obvious. This is where the attention mechanism comes in to
help. Before attention mechanisms, models struggled to keep track of which words
were most important in long sequences, often leading to poor predictions in tasks
like translation or question-answering.
The attention mechanism is like a spotlight that helps the machine figure out which
words in a sentence are most important for understanding the overall meaning.
Instead of looking at each word one by one without context, the attention mechanism
allows the model to pay attention to certain parts of the sentence based on their
relevance to each other.
Info
In simple terms, the attention mechanism helps a model look at the entire
input (like a sentence) and then decide which words to focus on when
generating an output (like a translation or a summary).
Now, let’s go step by step how the attention mechanism works under the hood.
Step 1: Understanding Each Word in the Sentence.
Imagine the machine is trying to understand the word it in the sentence: The ocean
is blue because it has no color itself.
• The word it is a bit tricky because, by itself, it doesn’t have much meaning.
We need to know what it refers to in the sentence.
• The machine looks at the other words in the sentence, like ocean, blue, color,
and tries to figure out which word is related to it.
Chapter 2: Deep dive into the theories of Generative AI 78
Step 2: Assigning Attention Scores.
The attention mechanism works by giving attention scores to each word based on
how related it is to it.
For example:
• Ocean gets a high attention (very important) score because it refers to ocean.
• Blue gets a low score (not as important) because it doesn’t refer to blue.
• Color might get a medium score (somewhat important) because it’s related
to ocean, but it’s not the word that it refers to.
The attention mechanism calculates these scores by comparing the relationships
between all the words in the sentence.
Figure 2.12
Step 3: Weighing the Words.
Chapter 2: Deep dive into the theories of Generative AI 79
Once the attention scores are assigned, the model uses them to adjust the influence
(or weight) of each word. Words with higher attention scores contribute more to the
model’s understanding of the sentence. Words with lower scores contribute less.
Now, the machine knows that the word ocean is the most important word to focus on
when it’s trying to understand the meaning of it. The attention mechanism directs
the model to pay more attention to ocean and less attention to other words like blue
or color.
This helps the machine correctly understand the sentence: The ocean is blue because
it has no color itself.
Step 4: Generating the Output.
After the model has figured out which words to focus on, it uses that information to
generate the output. This could be:
• Translating a sentence into another language.
• Summarizing a long text.
• Answering a question based on the input.
For instance, let’s say you’re using an AI translation tool to translate this sentence
from English to French. The attention mechanism will help the AI understand
that the word it refers to ocean, and make sure that the translation captures this
relationship properly. Instead of treating each word as equally important, the AI will
pay more attention to the important words like ocean and it to produce the correct
translation: L'océan est bleu parce qu'il n'a pas de couleur en lui-même..
In this section, we provide a high-level overview of how LLMs work internally
when processing NLP tasks. Behind the scenes, there’s a lot of complexity involved,
including decoding, matrix calculations, and weighting. To fully grasp these
nuances, a solid background in machine learning is required, which is beyond the
scope of this book. However, if you’re interested in learning the details, I highly
recommend starting with Jay Alammar’s blog post, The Illustrated Transformer.
Training LLM
Training is a crucial step in developing Large Language Models. It enables the
creation of powerful and versatile models that can handle a variety of tasks, from
generating general text to performing specialized functions.
Chapter 2: Deep dive into the theories of Generative AI 80
The training process is divided into two main stages: Pre-Training and Fine-Tuning.
Each stage plays a different role in preparing the model for its intended use.
Pre-Training lays the groundwork by exposing the model to a broad dataset and
general tasks. This stage helps the model understand the basics of how language
works.
Fine-Tuning, on the other hand, takes the pre-trained model and adapts it for specific
tasks or domains using specialized datasets. This stage refines the model’s abilities
to excel in particular applications.
Here’s a detailed look at these stages and their differences:
Pre-training
Pre-training is the initial phase where a large language model is trained on a broad
and diverse dataset to learn general language patterns, structures, and semantics.
This stage is designed to equip the model with a fundamental understanding of
language that can be applied across various tasks.
Let’s see an example. Imaging, a model might be trained using a dataset like
Wikipedia articles. For example, given the sentence The quick brown [MASK] jumps
over the lazy dog the model learns to predict the masked word, which could be
fox, by understanding the context provided by the surrounding words.
Pre-training an LLM involves training the model on a large, general-purpose dataset
to learn a wide range of language patterns. This process is typically done using:
• Masked Language Modeling: Randomly masking some input tokens and
predicting their context. Very similar to our previous example.
• Next Sentence Prediction: Predicting whether two given sentences are adja-
cent in a text.
Pre-training enables the model to develop strong general knowledge about language,
including grammar, syntax, semantics, pragmatics, and discourse structure. This step
is often performed using large-scale datasets like:
1. BooksCorpus: A collection of approximately 800 million words.
Chapter 2: Deep dive into the theories of Generative AI 2. WikiText-103: A dataset containing Wikipedia articles.
81
Figure 2.13
The key aspects of Pre-Training stage:
• Objective: To develop a general understanding of language by exposing the
model to large amounts of text data.
• Data: Utilizes a vast and diverse corpus, often including books, articles,
websites, and other text sources.
• Training Tasks: Common pre-training tasks include: Masked Language Model-
ing or Sentence Reordering.
• Language Modeling: Predicting the next word in a sequence (autoregressive
models) or filling in missing words (masked language models).
• Contextual Understanding: Learning to understand and generate contextually
appropriate text.
Pre-training is extremely costly and requires vast amounts of data, often running
into millions of dollars. This high expense is one reason that open-source models
will surpass commercial ones, as most open-source LLMs were initially developed
by corporations that made significant investments in pre-training. For beginners,
it’s important to understand that you won’t be handling pre-training yourself.
Fine-tuning
Fine-tuning is the subsequent phase where the pre-trained model is further trained
on a more specific and often smaller dataset tailored to a particular task or domain.
Chapter 2: Deep dive into the theories of Generative AI 82
This stage refines the model’s abilities and adapts its general knowledge to specialized
applications.
For example, if the goal is to develop a model for medical question answering, you
would fine-tune the pre-trained model on a dataset of medical questions and answers.
This helps the model become more proficient in understanding and generating
medical-related text.
The fine-tuning process involves retraining the model using:
• Task-Specific Data: Replacing the general-purpose dataset with a smaller,
more specialized one containing relevant information.
• Different Output Layers: Adjusting the output layer to match the format of
the specific task (e.g., multiple choice questions for sentiment analysis).
Key aspects of Fine-Tuning:
• Objective: To adapt the model’s general language understanding to specific
tasks or domains.
• Data: Uses a targeted dataset relevant to the specific application, such as
customer reviews for sentiment analysis or medical texts for healthcare-related
tasks.
• Training Tasks: Fine-tuning focuses on specific objectives, such as question
answering.
• Classification: Assigning labels to text (e.g., sentiment classification).
• Summarization: Generating summaries of longer documents.
• Translation: Translating text from one language to another.
Typically, when adapting an LLM for a specific domain in your company or for
personal use, you’ll use fine-tuning rather than training an LLM from scratch. We’ve
dedicated an entire chapter (chapter 5) of the book to explaining the fine-tuning
process with examples.
For now, let’s look at the key differences between these two stages:
Chapter 2: Deep dive into the theories of Generative AI 83
Scope Pre-Training Fine-Tuning
Purpose Aims to provide the
model with a broad
and general
understanding of
language. It is
designed to be a
foundational step that
prepares the model for
a wide range of tasks.
Focuses on adapting
the model to perform
well on specific tasks
or within particular
domains. It refines the
model’s capabilities
based on specialized
data.
Data Uses a large and
diverse dataset
covering many topics
and styles to build a
general language
model.
Uses a smaller,
domain-specific
dataset that is relevant
to the particular
application or task.
Training Objectives Involves tasks like
predicting the next
word or filling in
missing words to build
a general
understanding of
language.
Involves tasks such as
classification or
summarization to
tailor the model’s
performance to
specific applications.
Data Size Requires large-scale
datasets to capture a
wide range of
language patterns.
Uses smaller, targeted
datasets that are
specific to the desired
application.
Model Adaptation Establishes a broad
base of knowledge and
linguistic ability.
Adapts and refines this
knowledge to improve
performance on
particular tasks or in
specific domains.
Together, pre-training and fine-tuning enable the development of powerful and
versatile language models that can be applied to a wide range of tasks, from general
text generation to domain-specific applications. In the following sections of the
chapter, we will explore other related aspects of LLMs.
Chapter 2: Deep dive into the theories of Generative AI 84
RAG
RAG (Retrieval-Augmented Generation) and LLM Fine-Tuning are both techniques
used to enhance the capabilities of language models, but they serve different purposes
and operate in fundamentally different ways.
RAG is a hybrid approach that combines the strengths of retrieval-based methods
with generative language models. It involves two main components:
• Retrieval Module: This component searches a large external knowledge base
(such as a private Confluence, private cloud document storage, or any custom
database) to find the most relevant pieces of information based on the input
query.
• Generative Module: This is typically a pre-trained language model, such as
LLaMA or GPT, which takes the retrieved information along with the original
query to generate a coherent and informative response.
Instead of fine-tuning the model, you’ll often use RAG to enhance the LLM with
your private dataset. The RAG workflow is quite straightforward:
1. When you provide an input to an RAG model, it first retrieves relevant
information from a large database or knowledge graph using the retrieval
model. The retrieved information is then used to augment the input provided
to the model.
2. The generator model then uses this augmented input to generate text based
on your specific requirements. This process allows RAG models to leverage
existing knowledge and provide more accurate and informative responses.
Chapter 2: Deep dive into the theories of Generative AI 85
Figure 2.18
Key aspects of RAG:
• Domain-Specific Knowledge: RAG models are designed to cover a wide range
of topics and deliver more accurate, up-to-date responses.
• Knowledge Graph Integration: They often use extensive knowledge graphs
from various data sources, providing a more structured and comprehensive
understanding of information.
• Cost-Effectiveness: RAG is a highly efficient way to enhance an LLM in terms
of both development and hardware costs.
Understanding the differences between RAG and LLM fine-tuning is crucial for
selecting the best approach based on your application’s needs. Let’s go over the
key distinctions between the two methods:
Chapter 2: Deep dive into the theories of Generative AI 86
Feature RAG LLM Fine-Tuning
Mechanism Combines retrieval from
an external source with
generation.
Modifies model weights
(parameters) using
task-specific data.
Data Source Uses an external
knowledge base during
inference.
Relies on the training data
used for fine-tuning.
Adaptability Dynamically integrates
new information without
retraining.
Requires retraining to
adapt to new information.
Use Cases Fact-based QA, dynamic
knowledge integration.
Task-specific applications
like sentiment analysis,
translation.
Cost Low cost of developments High cost of training LLM
In summary, RAG is ideal for applications where the model needs to access and
integrate external, potentially dynamic information in real time. We assign an entire
chapter to discussing RAG and provide step-by-step instructions for developing a
RAG system tailored for a private company.
AI Agents
AI agents is a broad term that refers to autonomous or semi-autonomous systems
designed to perform specific tasks or solve problems by perceiving their environment,
processing information, making decisions, and taking actions to achieve goals. Such
agents can be used in automobile manufacturing or to control nuclear reactors.
In the context of Generative AI and LLMs, AI agents are sophisticated systems that
leverage generative models like LLaMA, Mistral, and GPT-4 to perform complex
tasks involving understanding, generating, and interacting with natural language.
These agents are designed to simulate human-like interactions, automate decision-
making processes, and facilitate various applications, from content generation to
personalized user experiences. Our main focus is on using generative AI agents to
simplify daily tasks, though they are likely to be applied in other sectors soon.
Imagine you’re planning a solo bicycle tour in the Republic of Seychelles, an
Chapter 2: Deep dive into the theories of Generative AI 87
archipelago in the Indian Ocean. While you could hire a tour operator to organize
the entire trip, you’re working with a limited budget and time. So, you decide to
ask your favorite LLM to create a solo tour plan for you. The LLM might provide a
high-level, day-by-day itinerary based on the information it has.
However, you need a more detailed plan that includes weather forecasts and
recommendations for overnight stays. You could search Google and go through all
the pages to gather this information yourself, but it would be time-consuming and
tedious.
Instead, you can use an AI agent to create a detailed bicycle tour plan for the
Seychelles. This agent can be configured to use search engines for up-to-date
information on the region, experiences of other cyclists, seasonal weather conditions,
and available flight options. The AI agent can then consolidate this information and
use the LLM to generate a comprehensive tour plan for you. While the plan may
not be perfect, it will provide a solid starting point that you can further customize as
needed.
The example above may not be perfect, but it provides a good understanding of what
an AI Agent can do. This integration of a general-purpose LLM with specialized AI
agents is what we now call an LLM Agent, which combines various capabilities to
deliver more comprehensive and relevant responses to user queries.
The primary purposes of an AI agents in Generative AI include:
• Enhanced User Interaction: AI agents can serve as advanced conversational
agents or virtual assistants, providing more natural and engaging interactions
with users. Example, an AI assistant fine-tuned on medical data to provide
healthcare-related advice.
• Automation of Complex Processes: These agents can automate complex
workflows by understanding user instructions, executing tasks in sequence, and
reporting back results. Example, AI tool that generates marketing copy, blog
posts, or social media content based on input prompts.
• Dynamic Knowledge Integration: AI agents can access external data sources
in real time, allowing them to provide up-to-date and accurate information
beyond the static knowledge encoded in their training data. Example, an agent
framework where different modules handle text generation, API calls, and user
interface management separately.
Chapter 2: Deep dive into the theories of Generative AI 88
• Personalization: AI agents can learn user preferences and behaviors over time,
enabling them to offer personalized recommendations or assistance tailored to
individual needs. Example, a customer service bot that improves its responses
based on user interactions and feedback.
• Decision Support and Problem Solving: AI agents can analyze large volumes
of data, identify patterns, and provide insights or recommendations/summa-
rization, assisting in decision-making processes. Example, an agent that an-
swers technical queries by pulling data from scientific databases and explaining
it in natural language.
The architecture of an LLM Agent consists of four main components:
1. Brain: The core of the agent, typically a LLM that acts as the decision-making
center.
2. Planning: This component breaks down complex tasks into manageable steps.
3. Memory: The agent stores context and information for future reference,
helping maintain continuity.
4. Tools: The agent utilizes various tools, such as API calls, database queries, and
even code execution, to complete tasks as configured by the user.
Figure 2.19 below provides a high-level overview of an LLM Agent.
Chapter 2: Deep dive into the theories of Generative AI 89
Figure 2.15
Please note that we haven’t covered all aspects of the LLM Agent in this section, as
there is an entire chapter dedicated to this topic. LLM Agents have a broad scope
and are expected to become key tools for managing routine tasks in the near future,
similar to how Alexa, Siri, and other home automation assistants are used today.
Personally, I frequently use agents in my daily workflow for tasks such as summa-
rizing technical documents, improving the grammar and clarity of emails, creating
content for social media, and even generating boilerplate code for MVP projects.
These agents are not just useful for personal tasks but can also be valuable for team
collaboration, enhancing productivity by generating high-quality documentation
from the codebase, improving programming code, and detecting bugs, among other
tasks. The potential of these agents is immense, and it’s important to pay close
attention to these tools as they develop.
Chapter 2: Deep dive into the theories of Generative AI 90
Prompt engineering
This chapter would be incomplete without discussing one of the crucial aspects of
maximizing the potential of Generative AI: prompt engineering. Prompt Engineer-
ing is the practice of designing and refining prompts to effectively communicate
instructions to language models like Llama, GPT, guiding them to generate desired
responses. Since these models interpret and respond based on the text input they
receive, the quality and structure of a prompt significantly impact the model’s output.
Why is prompt engineering essential? Because it offers the following benefits:
• Precision in output: A well-crafted prompt ensures that the model understands
the context and delivers accurate and relevant responses.
• Efficiency: By minimizing trial and error, prompt engineering saves time and
computational resources.
• Controlling behavior: It allows users to control the tone, style, and format of
the responses, which is particularly useful in applications like content creation,
customer support, and code generation.
• Reducing errors: Clear prompts help in reducing misunderstandings, biases,
and errors in the model’s outputs.
Let’s go by a few examples:
1. Content Generation:
Scenario: Suppose you want to generate a blog post about “The Benefits of
Renewable Energy.”
Basic Prompt: Write about renewable energy.
Engineered Prompt: Write a detailed 500-word blog post highlighting
the environmental, economic, and social benefits of renewable energy,
including examples such as solar and wind power.
Result: The engineered prompt provides a clearer framework for the model,
resulting in a more structured and informative article that covers the required
aspects.
Chapter 2: Deep dive into the theories of Generative AI 91
2. Code Generation:
Scenario: Writing a Python function to calculate the factorial of a number.
Basic Prompt: Write a Python function for factorial.
Engineered Prompt: Write a Python function called factorial that takes
an integer n as input and returns the factorial of n. Include
error handling for negative inputs and add a docstring explaining
the function.
Result: The engineered prompt results in a well-documented function with
error handling, making the code more robust and user-friendly.
There are several techniques in prompt engineering to achieve the desired result:
1. Role Assignment: Assigning a role or persona to the model, such as You are
a travel guide, or You are an editor helps in setting the context.
2. Explicit Instructions: Clearly stating the format, style, or steps to be followed
improves the response quality.
Example: You are an editor who receives a blog post; Your goal is
to review the blog post; to ensure that it follows journalistic best
practices
3. Chaining: Breaking down complex prompts into a sequence of simpler
prompts, allowing the model to tackle each step systematically.
Example: Proofread for grammatical errors and alignment with the
brand's voice.
4. Few-Shot Learning: Providing examples within the prompt to guide the model
on how to structure its response.
Example: Outcome should be in markdown format
To summarize, prompt engineering is essential because it bridges the gap between
user intent and model response, enhancing the utility and reliability of language
models across diverse applications.
Chapter 2: Deep dive into the theories of Generative AI 92
Resources
While writing these chapters, we referred to numerous textbooks, blogs, and
YouTube videos. The most memorable resources are listed below. If any resource
has been omitted, it was purely accidental and due to human error.
1. Attention Is All You Need https://arxiv.org/html/1706.03762v7
2. Natural Language Processing with Transformers, Book: https://transformersbook.com
3. What is NLP https://www.analyticsvidhya.com/blog/2020/05/what-is-
tokenization-nlp/
4. Word embedding: https://en.wikipedia.org/wiki/Word_embedding
5. Text similarity search in Elasticsearch using vector fields | Elastic
Blog: https://www.elastic.co/blog/text-similarity-search-with-vectors-in-
elasticsearch
6. Comparison Between Bagof Words and Word2Vec — PyImageSearch:
https://pyimagesearch.com/2022/07/18/comparison-between-bagofwords-
and-word2vec/
7. Word, Subword, and Character-Based Tokenization: Know the Difference:
https://towardsdatascience.com/word-subword-and-character-based-
tokenization-know-the-difference-ea0976b64e17
8. Word embeddings: https://www.tensorflow.org/text/guide/word_embeddings
9. The amazing power of word vectors: https://blog.acolyer.org/2016/04/21/the-
amazing-power-of-word-vectors/
10. Understanding NLP Word Embeddings — Text Vectorization:
https://towardsdatascience.com/understanding-nlp-word-embeddings-text-
vectorization-1a23744f7223
11. What Are Generative AI, Large Language Models, and Foundation Models?
https://cset.georgetown.edu/article/what-are-generative-ai-large-language-
models-and-foundation-models/
Conclusion
In this chapter, we took a deep dive into the fascinating world of Generative Artificial
Intelligence (Gen AI). We explored the fundamental concepts that differentiate Gen
Chapter 2: Deep dive into the theories of Generative AI 93
AI from traditional AI systems and clarified the terminology and jargon associated
with this rapidly evolving field.
We established that generative models are not just predictive machines; they are
creative engines that generate original content by learning patterns and structures
from vast datasets. This unique capability allows Gen AI to mimic human creativity,
produce novel outputs, and even surpass human abilities in specific areas.
As we continue our journey through this book, it’s important to remember that
understanding GenAI theories is only the beginning. The true power of Gen
AI lies in its applications—from automating creative tasks and personalizing user
experiences to accelerating scientific discovery and expanding the boundaries of
artistic expression.
In the upcoming chapters, we will explore practical examples of how to build and
use generative models, examining their potential impact across various industries.
We invite you to join us on this exciting journey as we uncover the secrets of Gen
AI and unlock its full potential.